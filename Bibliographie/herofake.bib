@inproceedings{10.1145/3369583.3392679,
  author    = {Yeh, Ting-An and Chen, Hung-Hsin and Chou, Jerry},
  title     = {{K}ube{S}hare: {A} Framework to Manage {GPU}s as First-Class and Shared Resources in Container Cloud},
  year      = {2020},
  isbn      = {9781450370523},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3369583.3392679},
  doi       = {10.1145/3369583.3392679},
  abstract  = {Container has emerged as a new technology in clouds to replace virtual machines~(VM) for distributed applications deployment and operation. With the increasing number of new cloud-focused applications, such as deep learning and high performance applications, started to reply on the high computing throughput of GPUs, efficiently supporting GPU in container cloud becomes essential. While GPU virtualization has been extensively studied for VM, limited work has been done for containers. One of the key challenges is the lack of support for GPU sharing between multiple concurrent containers. This limitation leads to low resource utilization when a GPU device cannot be fully utilized by a single application due to the burstiness of GPU workload and the limited memory bandwidth. To overcome this issue, we designed and implemented KubeShare, which extends Kubernetes to enable GPU sharing with fine-grained allocation. KubeShare is the first solution for Kubernetes to make GPU device as a first class resources for scheduling and allocations. Using real deep learning workloads, we demonstrated KubeShare can significantly increase GPU utilization and overall system throughput around 2x with less than 10\% performance overhead during container initialization and execution.},
  booktitle = {Proceedings of the 29th International Symposium on High-Performance Parallel and Distributed Computing},
  pages     = {173–184},
  numpages  = {12},
  keywords  = {container, scheduling, GPU, cloud computing},
  location  = {Stockholm, Sweden},
  series    = {HPDC '20}
}

@inproceedings{10.1145/3445814.3446699,
  author    = {Zha, Yue and Li, Jing},
  title     = {When Application-Specific {ISA} Meets {FPGA}s: {A} Multi-Layer Virtualization Framework for Heterogeneous Cloud {FPGA}s},
  year      = {2021},
  isbn      = {9781450383172},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3445814.3446699},
  doi       = {10.1145/3445814.3446699},
  abstract  = {While field-programmable gate arrays (FPGAs) have been widely deployed into cloud platforms, the high programming complexity and the inability to manage FPGA resources in an elastic/scalable manner largely limits the adoption of FPGA acceleration. Existing FPGA virtualization mechanisms partially address these limitations. Application-specific (AS) ISA provides a nice abstraction to enable a simple software programming flow that makes FPGA acceleration accessible by the mainstream software application developers. Nevertheless, existing AS ISA-based approaches can only manage FPGA resources at a per-device granularity, leading to a low resource utilization. Alternatively, hardware-specific (HS) abstraction improves the resource utilization by spatially sharing one FPGA among multiple applications. But it cannot reduce the programming complexity due to the lack of a high-level programming model. In this paper, we propose a virtualization mechanism for heterogeneous cloud FPGAs that combines AS ISA and HS abstraction to fully address aforementioned limitations. To efficiently combine these two abstractions, we provide a multi-layer virtualization framework with a new system abstraction as an indirection layer between them. This indirection layer hides the FPGA-specific resource constraints and leverages parallel pattern to effectively reduce the mapping complexity. It simplifies the mapping process into two steps, where the first step decomposes an AS ISA-based accelerator under no resource constraint to extract all fine-grained parallel patterns, and the second step leverages the extracted parallel patterns to simplify the process of mapping the decomposed accelerators onto the underlying HS abstraction. While system designers might be able to manually perform these steps for small accelerator designs, we develop a set of custom tools to automate this process and achieve a high mapping quality. By hiding FPGA-specific resource constraints, the proposed system abstraction provides a homogeneous view for the heterogeneous cloud FPGAs to simplify the runtime resource management. The extracted parallel patterns could also be leveraged by the runtime system to improve the performance of scale-out acceleration by maximally hiding the inter-FPGA communication latency. We use an AS ISA similar to the one proposed in BrainWave project and a recently proposed HS abstraction as a case study to demonstrate the effectiveness of the proposed virtualization framework. The performance is evaluated on a custom-built FPGA cluster with heterogeneous FPGA resources. Compared with the baseline system that only uses AS ISA, the proposed framework effectively combines these two abstractions and improves the aggregated system throughput by 2.54\texttimes{} with a marginal virtualization overhead.},
  booktitle = {Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages     = {123–134},
  numpages  = {12},
  keywords  = {Heterogeneous cloud FPGAs, Parallel patterns, Virtualization, Application-specific ISA},
  location  = {Virtual, USA},
  series    = {ASPLOS '21}
}

@inproceedings{5272532,
  author    = {Asano, Shuichi and Maruyama, Tsutomu and Yamaguchi, Yoshiki},
  booktitle = {2009 International Conference on Field Programmable Logic and Applications},
  title     = {Performance comparison of {FPGA}, {GPU} and {CPU} in image processing},
  year      = {2009},
  volume    = {},
  number    = {},
  pages     = {126-131},
  url       = {https://doi.org/10.1109/FPL.2009.5272532},
  doi       = {10.1109/FPL.2009.5272532}
}

@article{7279063,
  author  = {Dayarathna, Miyuru and Wen, Yonggang and Fan, Rui},
  journal = {IEEE Communications Surveys \& Tutorials},
  title   = {Data Center Energy Consumption Modeling: {A} Survey},
  year    = {2016},
  volume  = {18},
  number  = {1},
  pages   = {732-794},
  url     = {https://doi.org/10.1109/COMST.2015.2481183},
  doi     = {10.1109/COMST.2015.2481183}
}

@inproceedings{8782524,
  author    = {Qasaimeh, Murad and Denolf, Kristof and Lo, Jack and Vissers, Kees and Zambreno, Joseph and Jones, Phillip H.},
  booktitle = {2019 IEEE International Conference on Embedded Software and Systems (ICESS)},
  title     = {Comparing Energy Efficiency of {CPU}, {GPU} and {FPGA} Implementations for Vision Kernels},
  year      = {2019},
  volume    = {},
  number    = {},
  pages     = {1-8},
  url       = {https://doi.org/10.1109/ICESS.2019.8782524},
  doi       = {10.1109/ICESS.2019.8782524}
}

@inproceedings{8807741,
  author    = {Elhassouny, Azeddine and Smarandache, Florentin},
  booktitle = {2019 International Conference of Computer Science and Renewable Energies (ICCSRE)},
  title     = {Trends in Deep Convolutional Neural Networks Architectures: {A} Review},
  year      = {2019},
  volume    = {},
  number    = {},
  pages     = {1-8},
  url       = {https://doi.org/10.1109/ICCSRE.2019.8807741},
  doi       = {10.1109/ICCSRE.2019.8807741}
}

@article{9195730,
  author  = {Jahanshahi, Ali and Sabzi, Hadi Zamani and Lau, Chester and Wong, Daniel},
  journal = {IEEE Computer Architecture Letters},
  title   = {{GPU-NEST}: {C}haracterizing Energy Efficiency of Multi-{GPU} Inference Servers},
  year    = {2020},
  volume  = {19},
  number  = {2},
  pages   = {139-142},
  url     = {https://doi.org/10.1109/LCA.2020.3023723},
  doi     = {10.1109/LCA.2020.3023723}
}

@article{biom10070984,
  author         = {Sukegawa, Shintaro and Yoshii, Kazumasa and Hara, Takeshi and Yamashita, Katsusuke and Nakano, Keisuke and Yamamoto, Norio and Nagatsuka, Hitoshi and Furuki, Yoshihiko},
  title          = {Deep Neural Networks for Dental Implant System Classification},
  journal        = {Biomolecules},
  volume         = {10},
  year           = {2020},
  number         = {7},
  article-number = {984},
  url            = {https://www.mdpi.com/2218-273X/10/7/984},
  pubmedid       = {32630195},
  issn           = {2218-273X},
  abstract       = {In this study, we used panoramic X-ray images to classify and clarify the accuracy of different dental implant brands via deep convolutional neural networks (CNNs) with transfer-learning strategies. For objective labeling, 8859 implant images of 11 implant systems were used from digital panoramic radiographs obtained from patients who underwent dental implant treatment at Kagawa Prefectural Central Hospital, Japan, between 2005 and 2019. Five deep CNN models (specifically, a basic CNN with three convolutional layers, VGG16 and VGG19 transfer-learning models, and finely tuned VGG16 and VGG19) were evaluated for implant classification. Among the five models, the finely tuned VGG16 model exhibited the highest implant classification performance. The finely tuned VGG19 was second best, followed by the normal transfer-learning VGG16. We confirmed that the finely tuned VGG16 and VGG19 CNNs could accurately classify dental implant systems from 11 types of panoramic X-ray images.},
  doi            = {10.3390/biom10070984}
}

@inproceedings{buyyaSLAorientedResourceProvisioning2011,
  title      = {{SLA}-oriented Resource Provisioning for Cloud Computing: {C}hallenges, Architecture, and Solutions},
  shorttitle = {{SLA}-oriented Resource Provisioning for Cloud Computing},
  booktitle  = {2011 {I}nternational {C}onference on {C}loud and {S}ervice {C}omputing},
  author     = {Buyya, Rajkumar and Garg, Saurabh Kumar and Calheiros, Rodrigo N.},
  year       = {2011},
  month      = dec,
  pages      = {1--10},
  publisher  = {{IEEE}},
  address    = {{Hong Kong, China}},
  url        = {https://doi.org/10.1109/CSC.2011.6138522},
  doi        = {10.1109/CSC.2011.6138522},
  abstract   = {Cloud computing systems promise to offer subscription-oriented, enterprise-quality computing services to users worldwide. With the increased demand for delivering services to a large number of users, they need to offer differentiated services to users and meet their quality expectations. Existing resource management systems in data centers are yet to support Service Level Agreement (SLA)-oriented resource allocation, and thus need to be enhanced to realize cloud computing and utility computing. In addition, no work has been done to collectively incorporate customer-driven service management, computational risk management, and autonomic resource management into a market-based resource management system to target the rapidly changing enterprise requirements of Cloud computing. This paper presents vision, challenges, and architectural elements of SLA-oriented resource management. The proposed architecture supports integration of marketbased provisioning policies and virtualisation technologies for flexible allocation of resources to applications. The performance results obtained from our working prototype system shows the feasibility and effectiveness of SLA-based resource provisioning in Clouds.},
  isbn       = {978-1-4577-1637-9 978-1-4577-1635-5 978-1-4577-1636-2},
  langid     = {english}
}

@article{chaurasiaComprehensiveSurveyEnergyaware2021,
  title    = {Comprehensive Survey on Energy-Aware Server Consolidation Techniques in Cloud Computing},
  author   = {Chaurasia, Nisha and Kumar, Mohit and Chaudhry, Rashmi and Verma, Om Prakash},
  year     = {2021},
  month    = oct,
  journal  = {The Journal of Supercomputing},
  volume   = {77},
  number   = {10},
  pages    = {11682--11737},
  issn     = {0920-8542, 1573-0484},
  url      = {https://doi.org/10.1007/s11227-021-03760-1},
  doi      = {10.1007/s11227-021-03760-1},
  abstract = {The objective of cloud computing is to provide seamless services using virtualization technology over the Internet to serve the Quality of Service (QoS)-driven end users requirements. In order to provide various services like Software as a Service, Platform as a Service, Infrastructure as a Service, the cloud server's datacenters are kept active, which have large electrical consumption. Due to improper utilization of resources, optimizing the servers energy consumption becomes a significant challenge for service vendors from environmental and economic perspectives as well. The challenge to provide services with a low energy consumption profile opens up a new dimension of optimized server use for intelligent management of resources (such as CPU/disk/memory), with reduced power consumption through server consolidation. This enables fewer active physical servers to provide the required services without compromising the QoS. This article presents a narrative recent advancement comprehensive as well as systematic survey of existing energy-efficient techniques along with their limitations and the challenges associated in implementing them.},
  langid   = {english}
}

@inproceedings{choSLADrivenMLInference,
  title     = {{SLA}-Driven ML Inference Framework for Clouds With Heterogeneous Accelerators},
  author    = {Cho, Junguk and Tootaghaj, Diman Zad and Cao, Lianjie and Sharma, Puneet},
  booktitle = {Proceedings of Machine Learning and Systems},
  volume    = {4},
  pages     = {20--32},
  year      = {2022},
  url       = {https://proceedings.mlsys.org/paper/2022/file/0777d5c17d4066b82ab86dff8a46af6f-Paper.pdf},
  abstract  = {The current design of Serverless computing frameworks assumes that all the requests and underlying compute hardware are homogeneous. This homogeneity assumption causes two challenges in running ML workloads like Deep Neural Network (DNN) inference services on these frameworks. Such workloads can have various request types and might require heterogeneous accelerators. First, existing serverless frameworks are thresholdbased and use simple query per second or CPU utilization as autoscaling rules, thus ignoring heterogeneous requests and accelerators, resulting in sub-optimal performance. Second, ignoring infrastructure heterogeneity for workload scheduling and inference request distribution can lead to further performance inefficiencies. To address these challenges, we propose SLA-aware ML Inference Framework, which is a novel application and hardwareaware serverless computing framework to manage ML (e.g., DNN) inference applications in a heterogeneous infrastructure. Our framework designs an intelligent autoscaling strategy by leveraging rich, precise workloadspecific metrics and heterogeneous GPU compute capability. We schedule functions on the suitable GPU accelerators and proportionally distribute inference requests to the deployed functions based on the autoscaling decision. In addition, our framework enables efficient shares of GPU accelerators with multiple functions to increase resource efficiency with minimal overhead. Unlike prior works, we use application-specific SLA metrics to make scheduling/autoscaling decisions. We implement a prototype of our framework based on the Knative serverless framework and evaluate its performance with various DNN models.},
  langid    = {english}
}

@techreport{cncf2018whitepaper,
  title       = {{CNCF} {WG}-{S}erverless Whitepaper v1.0},
  author      = {Owens, Ken},
  institution = {{Cloud Native Computing Foundation}},
  year        = 2018
}

@inproceedings{dartoisCuckooOpportunisticMapReduce2019,
  title      = {{C}uckoo: {O}pportunistic {M}ap{R}educe on Ephemeral and Heterogeneous Cloud Resources},
  shorttitle = {{C}uckoo},
  booktitle  = {2019 {{IEEE}} 12th {{International Conference}} on {{Cloud Computing}} ({{CLOUD}})},
  author     = {Dartois, Jean-Emile and B. Ribeiro, Heverson and Boukhobza, Jalil and Barais, Olivier},
  year       = {2019},
  month      = jul,
  pages      = {396--403},
  publisher  = {{IEEE}},
  address    = {{Milan, Italy}},
  url        = {https://doi.org/10.1109/CLOUD.2019.00070},
  doi        = {10.1109/CLOUD.2019.00070},
  abstract   = {Cloud infrastructures are generally overprovisioned for handling load peaks and node failures. However, the drawback of this approach is that a large portion of data center resources remains unused. In this paper, we propose a framework that leverages unused resources of data centers, which are ephemeral by nature, to run MapReduce jobs. Our approach allows: i) to run efficiently Hadoop jobs on top of heterogeneous Cloud resources, thanks to our data placement strategy, ii) to predict accurately the volatility of ephemeral resources, thanks to the quantile regression method, and iii) for avoiding the interference between MapReduce jobs and co-resident workloads, thanks to our reactive QoS controller. We have extended Hadoop implementation with our framework and evaluated it with three different data center workloads. The experimental results show that our approach divides Hadoop job execution time by up to 7 when compared to the standard Hadoop implementation.},
  isbn       = {978-1-72812-705-7},
  langid     = {english}
}

@article{dartoisInvestigatingMachineLearning2021,
  title    = {Investigating Machine Learning Algorithms for Modeling {SSD} {I}/{O} Performance for Container-Based Virtualization},
  author   = {Dartois, Jean-Emile and Boukhobza, Jalil and Knefati, Anas and Barais, Olivier},
  year     = {2021},
  month    = jul,
  journal  = {IEEE Transactions on Cloud Computing},
  volume   = {9},
  number   = {3},
  pages    = {1103--1116},
  issn     = {2168-7161, 2372-0018},
  doi      = {10.1109/TCC.2019.2898192},
  abstract = {One of the cornerstones of the cloud provider business is to reduce hardware resources cost by maximizing their utilization. This is done through smartly sharing processor, memory, network and storage, while fully satisfying SLOs negotiated with customers. For the storage part, while SSDs are increasingly deployed in data centers mainly for their performance and energy efficiency, their internal mechanisms may cause a dramatic SLO violation. In effect, we measured that I/O interference may induce a 10x performance drop. We are building a framework based on autonomic computing which aims to achieve intelligent container placement on storage systems by preventing bad I/O interference scenarios. One prerequisite to such a framework is to design SSD performance models that take into account interactions between running processes/containers, the operating system and the SSD. These interactions are complex. In this paper, we investigate the use of machine learning for building such models in a container based Cloud environment. We have investigated five popular machine learning algorithms along with six different I/O intensive applications and benchmarks. We analyzed the prediction accuracy, the learning curve, the feature importance and the training time of the tested algorithms on four different SSD models. Beyond describing modeling component of our framework, this paper aims to provide insights for cloud providers to implement SLO compliant container placement algorithms on SSDs. Our machine learning-based framework succeeded in modeling I/O interference with a median Normalized Root-MeanSquare Error (NRMSE) of 2.5\%.},
  langid   = {english}
}

@inproceedings{DBLP:journals/corr/SimonyanZ14a,
  author    = {Karen Simonyan and
               Andrew Zisserman},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1409.1556},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SimonyanZ14a.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@online{energy-price,
  author = {Eurostat},
  title  = {Electricity prices for non-household consumers},
  url    = {https://ec.europa.eu/eurostat/statistics-explained/index.php?title=Electricity_price_statistics#Electricity_prices_for_non-household_consumers},
  year   = 2022
}

@inproceedings{fuerstFaasCacheKeepingServerless2021,
  title      = {{F}aasCache: Keeping Serverless Computing Alive with Greedy-Dual Caching},
  shorttitle = {{F}aasCache},
  booktitle  = {Proceedings of the 26th {{ACM International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}},
  author     = {Fuerst, Alexander and Sharma, Prateek},
  year       = {2021},
  month      = apr,
  pages      = {386--400},
  publisher  = {{ACM}},
  address    = {{Virtual USA}},
  url        = {https://doi.org/10.1145/3445814.3446757},
  doi        = {10.1145/3445814.3446757},
  abstract   = {Functions as a Service (also called serverless computing) promises to revolutionize how applications use cloud resources. However, functions suffer from cold-start problems due to the overhead of initializing their code and data dependencies before they can start executing. Keeping functions alive and warm after they have finished execution can alleviate the cold-start overhead. Keep-alive policies must keep functions alive based on their resource and usage characteristics, which is challenging due to the diversity in FaaS workloads.},
  isbn       = {978-1-4503-8317-2},
  langid     = {english}
}

@inproceedings{gujaratiSwayamDistributedAutoscaling2017,
  title      = {{S}wayam: {D}istributed Autoscaling to Meet {SLA}s of Machine Learning Inference Services with Resource Efficiency},
  shorttitle = {{S}wayam},
  booktitle  = {Proceedings of the 18th {{ACM}}/{{IFIP}}/{{USENIX Middleware Conference}}},
  author     = {Gujarati, Arpan and Elnikety, Sameh and He, Yuxiong and McKinley, Kathryn S. and Brandenburg, Bj{\"o}rn B.},
  year       = {2017},
  month      = dec,
  pages      = {109--120},
  publisher  = {{ACM}},
  address    = {{Las Vegas Nevada}},
  url        = {https://doi.org/10.1145/3135974.3135993},
  doi        = {10.1145/3135974.3135993},
  abstract   = {Developers use Machine Learning (ML) platforms to train ML models and then deploy these ML models as web services for inference (prediction). A key challenge for platform providers is to guarantee response-time Service Level Agreements (SLAs) for inference workloads while maximizing resource e ciency. Swayam is a fully distributed autoscaling framework that exploits characteristics of production ML inference workloads to deliver on the dual challenge of resource e ciency and SLA compliance. Our key contributions are (1) model-based autoscaling that takes into account SLAs and ML inference workload characteristics, (2) a distributed protocol that uses partial load information and prediction at frontends to provision new service instances, and (3) a backend self-decommissioning protocol for service instances. We evaluate Swayam on 15 popular services that were hosted on a production ML-as-a-service platform, for the following service-speci c SLAs: for each service, at least 99\% of requests must complete within the response-time threshold. Compared to a clairvoyant autoscaler that always satis es the SLAs (i.e., even if there is a burst in the request rates), Swayam decreases resource utilization by up to 27\%, while meeting the service-speci c SLAs over 96\% of the time during a three hour window. Microsoft Azure's Swayam-based framework was deployed in 2016 and has hosted over 100,000 services.},
  isbn       = {978-1-4503-4720-4},
  langid     = {english}
}

@inproceedings{handaoui2020releaser,
  title        = {{R}e{L}ea{SER}: {A} Reinforcement Learning Strategy for Optimizing Utilization Of Ephemeral Cloud Resources},
  author       = {Mohamed Handaoui and Jean-Emile Dartois and Jalil Boukhobza and Olivier Barais and Laurent d'Orazio},
  booktitle    = {2020 IEEE International Conference on Cloud Computing Technology and Science (CloudCom)},
  year         = {2020},
  pages        = {1--8},
  organization = {IEEE}
}

@inproceedings{handaoui2020salamander,
  title     = {{S}alamander: {A} Holistic Scheduling of {M}ap{R}educe Jobs on Ephemeral Cloud Resources},
  author    = {Handaoui, Mohamed and Dartois, Jean-Emile and Lemarchand, Laurent and Boukhobza, Jalil},
  booktitle = {The 20th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)},
  pages     = {320--329},
  year      = {2020}
}

@inproceedings{hellersteinServerlessComputingOne2019,
  title     = {Serverless Computing: {O}ne Step Forward, Two Steps Back},
  author    = {Joseph M. Hellerstein and
               Jose M. Faleiro and
               Joseph Gonzalez and
               Johann Schleier{-}Smith and
               Vikram Sreekanti and
               Alexey Tumanov and
               Chenggang Wu},
  booktitle = {9th Biennial Conference on Innovative Data Systems Research, {CIDR}
               2019, Asilomar, CA, USA, January 13-16, 2019, Online Proceedings},
  publisher = {www.cidrdb.org},
  year      = {2019},
  url       = {http://cidrdb.org/cidr2019/papers/p119-hellerstein-cidr19.pdf},
  timestamp = {Tue, 23 Mar 2021 15:36:39 +0100},
  biburl    = {https://dblp.org/rec/conf/cidr/HellersteinFGSS19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  abstract  = {Serverless computing offers the potential to program the cloud in an autoscaling, pay-as-you go manner. In this paper we address critical gaps in first-generation serverless computing, which place its autoscaling potential at odds with dominant trends in modern computing: notably data-centric and distributed computing, but also open source and custom hardware. Put together, these gaps make current serverless offerings a bad fit for cloud innovation and particularly bad for data systems innovation. In addition to pinpointing some of the main shortfalls of current serverless architectures, we raise a set of challenges we believe must be met to unlock the radical potential that the cloud\textemdash with its exabytes of storage and millions of cores\textemdash should offer to innovative developers.},
  langid    = {english}
}

@inproceedings{hortaXartrekRuntimeExecution2021,
  title      = {{X}ar-{T}rek: {R}un-Time Execution Migration among {FPGA}s and Heterogeneous-{ISA} {CPU}s},
  shorttitle = {{X}ar-{T}rek},
  booktitle  = {Proceedings of the 22nd {{International Middleware Conference}}},
  author     = {Horta, Edson and Chuang, Ho-Ren and VSathish, Naarayanan Rao and Philippidis, Cesar and Barbalace, Antonio and Olivier, Pierre and Ravindran, Binoy},
  year       = {2021},
  month      = dec,
  pages      = {104--118},
  publisher  = {{ACM}},
  address    = {{Qu\'ebec city Canada}},
  url        = {https://doi.org/10.1145/3464298.3493388},
  doi        = {10.1145/3464298.3493388},
  abstract   = {Datacenter servers are increasingly heterogeneous: from x86 host CPUs, to ARM or RISC-V CPUs in NICs/SSDs, to FPGAs. Previous works have demonstrated that migrating application execution at run-time across heterogeneous-ISA CPUs can yield significant performance and energy gains, with relatively little programmer effort. However, FPGAs have often been overlooked in that context: hardware acceleration using FPGAs involves statically implementing select application functions, which prohibits dynamic and transparent migration. We present Xar-Trek, a new compiler and run-time software framework that overcomes this limitation. Xar-Trek compiles an application for several CPU ISAs and select application functions for acceleration on an FPGA, allowing execution migration between heterogeneous-ISA CPUs and FPGAs at run-time. Xar-Trek's run-time monitors server workloads and migrates application functions to an FPGA or to heterogeneous-ISA CPUs based on a scheduling policy. We develop a heuristic policy that uses application workload profiles to make scheduling decisions. Our evaluations conducted on a system with x86-64 server CPUs, ARM64 server CPUs, and an Alveo accelerator card reveal 88\%-1\% performance gains over no-migration baselines.},
  isbn       = {978-1-4503-8534-3},
  langid     = {english}
}

%%%%%

@misc{https://doi.org/10.48550/arxiv.1411.7766,
  doi       = {10.48550/ARXIV.1411.7766},
  url       = {https://arxiv.org/abs/1411.7766},
  author    = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Deep Learning Face Attributes in the Wild},
  publisher = {arXiv},
  year      = {2014},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{jimaging7080128,
  author         = {Giudice, Oliver and Guarnera, Luca and Battiato, Sebastiano},
  title          = {Fighting Deepfakes by Detecting {GAN} {DCT} Anomalies},
  journal        = {Journal of Imaging},
  volume         = {7},
  year           = {2021},
  number         = {8},
  article-number = {128},
  url            = {https://www.mdpi.com/2313-433X/7/8/128},
  issn           = {2313-433X},
  abstract       = {To properly contrast the Deepfake phenomenon the need to design new Deepfake detection algorithms arises; the misuse of this formidable A.I. technology brings serious consequences in the private life of every involved person. State-of-the-art proliferates with solutions using deep neural networks to detect a fake multimedia content but unfortunately these algorithms appear to be neither generalizable nor explainable. However, traces left by Generative Adversarial Network (GAN) engines during the creation of the Deepfakes can be detected by analyzing ad-hoc frequencies. For this reason, in this paper we propose a new pipeline able to detect the so-called GAN Specific Frequencies (GSF) representing a unique fingerprint of the different generative architectures. By employing Discrete Cosine Transform (DCT), anomalous frequencies were detected. The β statistics inferred by the AC coefficients distribution have been the key to recognize GAN-engine generated data. Robustness tests were also carried out in order to demonstrate the effectiveness of the technique using different attacks on images such as JPEG Compression, mirroring, rotation, scaling, addition of random sized rectangles. Experiments demonstrated that the method is innovative, exceeds the state of the art and also give many insights in terms of explainability.},
  doi            = {10.3390/jimaging7080128}
}

@article{jonasCloudProgrammingSimplified2019,
  title      = {Cloud Programming Simplified: {A} {B}erkeley View on Serverless Computing},
  author     = {Eric Jonas and
                Johann Schleier{-}Smith and
                Vikram Sreekanti and
                Chia{-}che Tsai and
                Anurag Khandelwal and
                Qifan Pu and
                Vaishaal Shankar and
                Jo{\~{a}}o Carreira and
                Karl Krauth and
                Neeraja Jayant Yadwadkar and
                Joseph E. Gonzalez and
                Raluca Ada Popa and
                Ion Stoica and
                David A. Patterson},
  journal    = {CoRR},
  volume     = {abs/1902.03383},
  year       = {2019},
  url        = {http://arxiv.org/abs/1902.03383},
  eprinttype = {arXiv},
  eprint     = {1902.03383},
  timestamp  = {Mon, 31 Jan 2022 16:34:15 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1902-03383.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  abstract   = {Serverless cloud computing handles virtually all the system administration operations needed to make it easier for programmers to use the cloud. It provides an interface that greatly simplifies cloud programming, and represents an evolution that parallels the transition from assembly language to high-level programming languages. This paper gives a quick history of cloud computing, including an accounting of the predictions of the 2009 Berkeley View of Cloud Computing paper, explains the motivation for serverless computing, describes applications that stretch the current limits of serverless, and then lists obstacles and research opportunities required for serverless computing to fulfill its full potential. Just as the 2009 paper identified challenges for the cloud and predicted they would be addressed and that cloud use would accelerate, we predict these issues are solvable and that serverless computing will grow to dominate the future of cloud computing.}
}

@inproceedings{khandelwalTaureauDeconstructingServerless2020,
  title      = {{L}e {T}aureau: {D}econstructing the Serverless Landscape \& A Look Forward},
  shorttitle = {{L}e {T}aureau},
  booktitle  = {Proceedings of the 2020 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author     = {Khandelwal, Anurag and Kejariwal, Arun and Ramasamy, Karthikeyan},
  year       = {2020},
  month      = jun,
  pages      = {2641--2650},
  publisher  = {{ACM}},
  address    = {{Portland OR USA}},
  url        = {https://doi.org/10.1145/3318464.3383130},
  doi        = {10.1145/3318464.3383130},
  abstract   = {Akin to the natural evolution of programming in assembly language to high-level languages, serverless computing represents the next frontier in the evolution of cloud computing: bare metal \textrightarrow{} virtual machines \textrightarrow{} containers \textrightarrow{} serverless. The genesis of serverless computing can be traced back to the fundamental need of enabling a programmer to singularly focus on writing application code in a high-level language and isolating all facets of system management (for example, but not limited to, instance selection, scaling, deployment, logging, monitoring, fault tolerance and so on). This is particularly critical in light of today's, increasingly tightening, time-to-market constraints. Currently, serverless computing is supported by leading public cloud vendors, such as AWS Lambda, Google Cloud Functions, Azure Cloud Functions and others. While this is an important step in the right direction, there are many challenges going forward. For instance, but not limited to, how to enable support for dynamic optimization, how to extend support for stateful computation, how to efficiently bin-pack applications, how to support hardware heterogeneity (this will be key especially in light of the emergence of hardware accelerators for deep learning workloads).},
  isbn       = {978-1-4503-6735-6},
  langid     = {english}
}

@online{knative-autoscaling,
  author = {The {K}native Authors},
  title  = {{K}native -- {A}utoscaling},
  url    = {https://github.com/knative/serving/tree/main/docs/scaling},
  year   = 2022
}

@online{knative-concurrency,
  author = {The {K}native Authors},
  title  = {{K}native -- {C}onfiguraing concurrency},
  url    = {https://knative.dev/docs/serving/autoscaling/concurrency/#soft-versus-hard-concurrency-limits},
  year   = 2022
}

@inproceedings{lingPigeonDynamicEfficient2019,
  title      = {{P}igeon: {A} Dynamic and Efficient Serverless and {FaaS} Framework for Private Cloud},
  shorttitle = {{P}igeon},
  booktitle  = {2019 {{International Conference}} on {{Computational Science}} and {{Computational Intelligence}} ({{CSCI}})},
  author     = {Ling, Wei and Ma, Lin and Tian, Chen and Hu, Ziang},
  year       = {2019},
  month      = dec,
  pages      = {1416--1421},
  publisher  = {{IEEE}},
  address    = {{Las Vegas, NV, USA}},
  url        = {https://doi.org/10.1109/CSCI49370.2019.00265},
  doi        = {10.1109/CSCI49370.2019.00265},
  abstract   = {Recently, voice-triggered small cloud functions such as Alexa skills, and cloud mini programs for IoT and smartphone, grow exponentially. These new developments also attract organizations to host their own cloud functions or mini programs in private cloud environment and move from traditional Microservice architecture to Serverless Function-as-a-Service (FaaS) architecture. However, current Serverless FaaS frameworks cannot meet cold start latency, resource efficiency required by shortlived cloud functions and mini programs.},
  isbn       = {978-1-72815-584-5},
  langid     = {english}
}

@inproceedings{mampageDeadlineawareDynamicResource2021,
  title     = {Deadline-Aware Dynamic Resource Management in Serverless Computing Environments},
  booktitle = {2021 {{IEEE}}/{{ACM}} 21st {{International Symposium}} on {{Cluster}}, {{Cloud}} and {{Internet Computing}} ({{CCGrid}})},
  author    = {Mampage, Anupama and Karunasekera, Shanika and Buyya, Rajkumar},
  year      = {2021},
  month     = may,
  pages     = {483--492},
  publisher = {{IEEE}},
  address   = {{Melbourne, Australia}},
  url       = {https://doi.org/10.1109/CCGrid51090.2021.00058},
  doi       = {10.1109/CCGrid51090.2021.00058},
  abstract  = {Serverless computing enables rapid application development and deployment by composing loosely coupled microservices at a scale. This emerging paradigm greatly unburdens the users of cloud environments, from the need to provision and manage the underlying cloud resources. With this shift in responsibility, the cloud provider faces the challenge of providing acceptable performance to the user without compromising on reliability, while having minimal knowledge of the application requirements. Sub-optimal resource allocations, specifically the CPU resources, could result in the violation of performance requirements of applications. Further, the fine-grained serverless billing model only charges for resource usage in terms of function execution time. At the same time, the provider has to maintain the underlying infrastructure in always-on mode to facilitate asynchronous function calls. Thus, achieving optimum utilization of cloud resources without compromising on application requirements is of high importance to the provider. Most of the current works only focus on minimizing function execution times caused by delays in infrastructure set up and reducing resource costs for the end-user. However, in this paper, we focus on both the provider and user's perspective and propose a function placement policy and a dynamic resource management policy for applications deployed in serverless computing environments. The policies minimize the resource consumption cost for the service provider while meeting the user's application requirement, i.e., deadline. The proposed solutions are sensitive to deadline and efficiently increase the resource utilization for the provider, while dynamically managing resources to improve function response times. We implement and evaluate our approach through simulation using ContainerCloudSim toolkit. The proposed function placement policy when compared with baseline scheduling techniques can reduce resource consumption by up to three times. The dynamic resource allocation policy when evaluated with a fixed resource allocation policy and a proportional CPU-shares policy shows improvements of up to 25\% in meeting the required function deadlines.},
  isbn      = {978-1-72819-586-5},
  langid    = {english}
}

%%%%%

@article{mampageHolisticViewResource2022,
  title      = {A Holistic View on Resource Management in Serverless Computing Environments: {T}axonomy and Future Directions},
  shorttitle = {A {{Holistic View}} on {{Resource Management}} in {{Serverless Computing Environments}}},
  author     = {Mampage, Anupama and Karunasekera, Shanika and Buyya, Rajkumar},
  year       = {2022},
  month      = jan,
  journal    = {ACM Computing Surveys},
  pages      = {3510412},
  issn       = {0360-0300, 1557-7341},
  url        = {https://doi.org/10.1145/3510412},
  doi        = {10.1145/3510412},
  abstract   = {Serverless computing has emerged as an attractive deployment option for cloud applications in recent times. The unique features of this computing model include rapid auto-scaling, strong isolation, ine-grained billing options and access to a massive service ecosystem which autonomously handles resource management decisions. This model is increasingly being explored for deployments in geographically distributed edge and fog computing networks as well, due to these characteristics. Efective management of computing resources has always gained a lot of attention among researchers. The need to automate the entire process of resource provisioning, allocation, scheduling, monitoring and scaling, has resulted in the need for specialized focus on resource management under the serverless model. In this article, we identify the major aspects covering the broader concept of resource management in serverless environments and propose a taxonomy of elements which inluence these aspects, encompassing characteristics of system design, workload attributes and stakeholder expectations. We take a holistic view on serverless environments deployed across edge, fog and cloud computing networks. We also analyse existing works discussing aspects of serverless resource management using this taxonomy. This article further identiies gaps in literature and highlights future research directions for improving capabilities of this computing model. CCS Concepts: {$\cdot$} General and reference \textrightarrow{} Surveys and overviews; {$\cdot$} Computer systems organization \textrightarrow{} Distributed architectures.},
  langid     = {english}
}

@inproceedings{mancoMyVMLighter2017,
  title     = {My {VM} is Lighter (and Safer) than Your Container},
  author    = {Manco, Filipe and Lupu, Costin and Schmidt, Florian and Mendes, Jose and Kuenzer, Simon and Sati, Sumit and Yasukata, Kenichi and Raiciu, Costin and Huici, Felipe},
  year      = {2017},
  isbn      = {9781450350853},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3132747.3132763},
  doi       = {10.1145/3132747.3132763},
  abstract  = {Containers are in great demand because they are lightweight when compared to virtual machines. On the downside, containers offer weaker isolation than VMs, to the point where people run containers in virtual machines to achieve proper isolation. In this paper, we examine whether there is indeed a strict tradeoff between isolation (VMs) and efficiency (containers). We find that VMs can be as nimble as containers, as long as they are small and the toolstack is fast enough.We achieve lightweight VMs by using unikernels for specialized applications and with Tinyx, a tool that enables creating tailor-made, trimmed-down Linux virtual machines. By themselves, lightweight virtual machines are not enough to ensure good performance since the virtualization control plane (the toolstack) becomes the performance bottleneck. We present LightVM, a new virtualization solution based on Xen that is optimized to offer fast boot-times regardless of the number of active VMs. LightVM features a complete redesign of Xen's control plane, transforming its centralized operation to a distributed one where interactions with the hypervisor are reduced to a minimum. LightVM can boot a VM in 2.3ms, comparable to fork/exec on Linux (1ms), and two orders of magnitude faster than Docker. LightVM can pack thousands of LightVM guests on modest hardware with memory and CPU usage comparable to that of processes.},
  booktitle = {Proceedings of the 26th Symposium on Operating Systems Principles},
  pages     = {218--233},
  numpages  = {16},
  keywords  = {Xen, containers, virtual machine, operating systems, hypervisor, specialization, unikernels, Virtualization},
  location  = {Shanghai, China},
  series    = {SOSP '17}
}

@inproceedings{mullerLambadaInteractiveData2020,
  title      = {{L}ambada: {I}nteractive Data Analytics on Cold Data Using Serverless Cloud Infrastructure},
  shorttitle = {{L}ambada},
  booktitle  = {Proceedings of the 2020 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  author     = {M{\"u}ller, Ingo and Marroqu{\'i}n, Renato and Alonso, Gustavo},
  year       = {2020},
  month      = jun,
  pages      = {115--130},
  publisher  = {{ACM}},
  address    = {{Portland OR USA}},
  url        = {https://doi.org/10.1145/3318464.3389758},
  doi        = {10.1145/3318464.3389758},
  abstract   = {Serverless computing has recently attracted a lot of attention from research and industry due to its promise of ultimate elasticity and operational simplicity. However, there is no consensus yet on whether or not the approach is suitable for data processing. In this paper, we present Lambada, a serverless distributed data processing framework designed to explore how to perform data analytics on serverless computing. In our analysis, supported with extensive experiments, we show in which scenarios serverless makes sense from an economic and performance perspective. We address several important technical questions that need to be solved to support data analytics and present examples from several domains where serverless offers a cost and performance advantage over existing solutions.},
  isbn       = {978-1-4503-6735-6},
  langid     = {english}
}

%%%%%

@inproceedings{NEURIPS2019_7716d0fc,
  author    = {Liu, Tianyi and Chen, Minshuo and Zhou, Mo and Du, Simon S and Zhou, Enlu and Zhao, Tuo},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Towards Understanding the Importance of Shortcut Connections in Residual Networks},
  url       = {https://proceedings.neurips.cc/paper/2019/file/7716d0fc31636914783865d34f6cdfd5-Paper.pdf},
  volume    = {32},
  year      = {2019}
}

@inproceedings{reissHeterogeneityDynamicityClouds,
  author    = {Reiss, Charles and Tumanov, Alexey and Ganger, Gregory R. and Katz, Randy H. and Kozuch, Michael A.},
  title     = {Heterogeneity and Dynamicity of Clouds at Scale: {G}oogle Trace Analysis},
  year      = {2012},
  isbn      = {9781450317610},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2391229.2391236},
  doi       = {10.1145/2391229.2391236},
  abstract  = {To better understand the challenges in developing effective cloud-based resource schedulers, we analyze the first publicly available trace data from a sizable multi-purpose cluster. The most notable workload characteristic is heterogeneity: in resource types (e.g., cores:RAM per machine) and their usage (e.g., duration and resources needed). Such heterogeneity reduces the effectiveness of traditional slot- and core-based scheduling. Furthermore, some tasks are constrained as to the kind of machine types they can use, increasing the complexity of resource assignment and complicating task migration. The workload is also highly dynamic, varying over time and most workload features, and is driven by many short jobs that demand quick scheduling decisions. While few simplifying assumptions apply, we find that many longer-running jobs have relatively stable resource utilizations, which can help adaptive resource schedulers.},
  booktitle = {Proceedings of the Third ACM Symposium on Cloud Computing},
  articleno = {7},
  numpages  = {13},
  location  = {San Jose, California},
  series    = {SoCC '12}
}

@article{SchleierSmith2021WhatSC,
  title      = {What Serverless Computing is and Should Become: {T}he next Phase of Cloud Computing},
  author     = {Schleier-Smith, Johann and Sreekanti, Vikram and Khandelwal, Anurag and Carreira, Joao and Yadwadkar, Neeraja J. and Popa, Raluca Ada and Gonzalez, Joseph E. and Stoica, Ion and Patterson, David A.},
  year       = {2021},
  issue_date = {May 2021},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {64},
  number     = {5},
  issn       = {0001-0782},
  url        = {https://doi.org/10.1145/3406011},
  doi        = {10.1145/3406011},
  abstract   = {The evolution that serverless computing represents, the economic forces that shape it, why it could fail, and how it might fulfill its potential.},
  journal    = {Commun. ACM},
  month      = {apr},
  pages      = {76–84},
  numpages   = {9}
}

@inproceedings{shahradServerlessWildCharacterizing,
  title     = {Serverless in the Wild: {C}haracterizing and Optimizing the Serverless Workload at a Large Cloud Provider},
  author    = {Shahrad, Mohammad and Fonseca, Rodrigo and Goiri, {\'I}{\~n}igo and Chaudhry, Gohar and Batum, Paul and Cooke, Jason and Laureano, Eduardo and Tresness, Colby and Russinovich, Mark and Bianchini, Ricardo},
  pages     = {14},
  abstract  = {Function as a Service (FaaS) has been gaining popularity as a way to deploy computations to serverless backends in the cloud. This paradigm shifts the complexity of allocating and provisioning resources to the cloud provider, which has to provide the illusion of always-available resources (i.e., fast function invocations without cold starts) at the lowest possible resource cost. Doing so requires the provider to deeply understand the characteristics of the FaaS workload. Unfortunately, there has been little to no public information on these characteristics. Thus, in this paper, we first characterize the entire production FaaS workload of Azure Functions. We show for example that most functions are invoked very infrequently, but there is an 8-order-of-magnitude range of invocation frequencies. Using observations from our characterization, we then propose a practical resource management policy that significantly reduces the number of function cold starts, while spending fewer resources than state-of-the-practice policies.},
  langid    = {english},
  year      = {2020},
  url       = {https://www.usenix.org/conference/atc20/presentation/shahrad},
  isbn      = {978-1-939133-14-4},
  publisher = {USENIX Association},
  address   = {USA},
  booktitle = {Proceedings of the 2020 USENIX Conference on Usenix Annual Technical Conference},
  articleno = {14},
  numpages  = {14},
  series    = {USENIX ATC'20}
}

@techreport{shehabiUnitedStatesData2016,
  title       = {{U}nited {S}tates Data Center Energy Usage Report},
  author      = {Shehabi, Arman and Smith, Sarah and Sartor, Dale and Brown, Richard and Herrlin, Magnus and Koomey, Jonathan and Masanet, Eric and Horner, Nathaniel and Azevedo, In{\^e}s and Lintner, William},
  institution = {Lawrence Berkeley National Lab},
  year        = {2016},
  month       = jun,
  number      = {LBNL--1005775, 1372902},
  pages       = {LBNL--1005775, 1372902},
  doi         = {10.2172/1372902},
  langid      = {english}
}

@online{simpy,
  author = {Team {SimPy}},
  title  = {{SimPy}},
  url    = {https://simpy.readthedocs.io/},
  year   = 2022
}

@inproceedings{singhviAtollScalableLowLatency2021,
  title      = {{A}toll: {A} Scalable Low-Latency Serverless Platform},
  shorttitle = {{A}toll},
  booktitle  = {Proceedings of the {{ACM Symposium}} on {{Cloud Computing}}},
  author     = {Singhvi, Arjun and Balasubramanian, Arjun and Houck, Kevin and Shaikh, Mohammed Danish and Venkataraman, Shivaram and Akella, Aditya},
  year       = {2021},
  month      = nov,
  pages      = {138--152},
  publisher  = {{ACM}},
  address    = {{Seattle WA USA}},
  url        = {https://doi.org/10.1145/3472883.3486981},
  doi        = {10.1145/3472883.3486981},
  abstract   = {With user-facing apps adopting serverless computing, good latency performance of serverless platforms has become a strong fundamental requirement. However, it is di\dbend cult to achieve this on platforms today due to the design of their underlying control and data planes that are particularly illsuited to short-lived functions with unpredictable arrival patterns. We present Atoll, a serverless platform, that overcomes the challenges via a ground-up redesign of the control and data planes. In Atoll, each app is associated with a latency deadline. Atoll achieves its per-app request latency goals by: (a) partitioning the cluster into (semi-global scheduler, worker pool) pairs, (b) performing deadline-aware scheduling and proactive sandbox allocation, and (c) using a load balancing layer to do sandbox-aware routing, and automatically scale the semi-global schedulers per app. Our results show that Atoll reduces missed deadlines by {$\dashleftarrow$}66{$\RightArrowBar$} and tail latencies by {$\dashleftarrow$}3{$\RightArrowBar$} compared to state-of-the-art alternatives.},
  isbn       = {978-1-4503-8638-8},
  langid     = {english}
}

@inproceedings{sureshENSUREEfficientScheduling2020,
  title      = {{ENSURE}: {E}fficient Scheduling and Autonomous Resource Management in Serverless Environments},
  shorttitle = {{ENSURE}},
  booktitle  = {2020 {{IEEE International Conference}} on {{Autonomic Computing}} and {{Self-Organizing Systems}} ({{ACSOS}})},
  author     = {Suresh, Amoghavarsha and Somashekar, Gagan and Varadarajan, Anandh and Kakarla, Veerendra Ramesh and Upadhyay, Hima and Gandhi, Anshul},
  year       = {2020},
  month      = aug,
  pages      = {1--10},
  publisher  = {{IEEE}},
  address    = {{Washington, DC, USA}},
  url        = {https://doi.org/10.1109/ACSOS49614.2020.00020},
  doi        = {10.1109/ACSOS49614.2020.00020},
  abstract   = {An imminent challenge in the serverless computing landscape is the escalating cost of infrastructure needed to handle the growing traffic at scale. This work presents ENSURE, a function-level scheduler and autonomous resource manager designed to minimize provider resource costs while meeting customer performance requirements. ENSURE works by classifying incoming function requests at runtime and carefully regulating the resource usage of colocated functions on each invoker. Beyond a single invoker, ENSURE elastically scales capacity, using concepts from operations research, in response to varying workload traffic to prevent cold starts. Finally, ENSURE schedules requests by concentrating load on an adequate number of invokers to encourage reuse of active hosts (thus further avoiding cold starts) and allow unneeded capacity to provably and gracefully time out. We implement ENSURE on Apache OpenWhisk and show that, across several serverless applications and compared to existing baselines, ENSURE significantly improves resource efficiency, by as much as 52\%, while providing acceptable application latency.},
  isbn       = {978-1-72817-277-4},
  langid     = {english}
}

@inproceedings{vahidiniaColdStartServerless2020,
  title      = {Cold Start in Serverless Computing: {C}urrent Trends and Mitigation Strategies},
  shorttitle = {Cold {{Start}} in {{Serverless Computing}}},
  booktitle  = {2020 {{International Conference}} on {{Omni-layer Intelligent Systems}} ({{COINS}})},
  author     = {Vahidinia, Parichehr and Farahani, Bahar and Aliee, Fereidoon Shams},
  year       = {2020},
  month      = aug,
  pages      = {1--7},
  publisher  = {{IEEE}},
  address    = {{Barcelona, Spain}},
  url        = {https://doi.org/10.1109/COINS49042.2020.9191377},
  doi        = {10.1109/COINS49042.2020.9191377},
  abstract   = {Serverless Computing is the latest cloud computing model, which facilitates application development. By adopting and leveraging the modern paradigm of Serverless Computing, developers do not need to manage the servers. In this computational model, the executables are independent functions that are individually deployed on a Serverless platform offering instant per-request elasticity. Such elasticity typically comes at the cost of the "Cold Starts" problem. This phenomenon is associated with a delay occurring due to provision a runtime container to execute the functions. Shortly after Amazon introduced this computing model with the AWS Lambda platform in 2014, several open source and commercial platforms also started embracing and offering this technology. Each platform has its own solution to deal with Cold Starts. The evaluation of the performance of each platform under the load and factors influencing the cold start problem has received much attention over the past few years. This paper provides a comprehensive overview on the recent advancements and stateof-the-art works in mitigating the cold start delay. Moreover, several sets of experiments have been performed to study the behavior of the AWS Lambda as the base platform with respect to the cold start delay.},
  isbn       = {978-1-72816-371-0},
  langid     = {english}
}

@inproceedings{vaneykSPECRGCloud2018,
  title     = {A {SPEC} {RG} Cloud Group's Vision on the Performance Challenges of {FaaS} Cloud Architectures},
  booktitle = {Companion of the 2018 {{ACM}}/{{SPEC International Conference}} on {{Performance Engineering}}},
  author    = {{van Eyk}, Erwin and Iosup, Alexandru and Abad, Cristina L. and Grohmann, Johannes and Eismann, Simon},
  year      = {2018},
  month     = apr,
  pages     = {21--24},
  publisher = {{ACM}},
  address   = {{Berlin Germany}},
  url       = {https://doi.org/10.1145/3185768.3186308},
  doi       = {10.1145/3185768.3186308},
  abstract  = {As a key part of the serverless computing paradigm, Function-asa-Service (FaaS) platforms enable users to run arbitrary functions without being concerned about operational issues. However, there are several performance-related issues surrounding the state-ofthe-art FaaS platforms that can deter widespread adoption of FaaS, including sizeable overheads, unreliable performance, and new forms of the cost-performance trade-off. In this work we, the SPEC RG Cloud Group, identify six performance-related challenges that arise specifically in this FaaS model, and present our roadmap to tackle these problems in the near future. This paper aims at motivating the community to solve these challenges together.},
  isbn      = {978-1-4503-5629-9},
  langid    = {english}
}

@online{vitis-ai,
  author = {Xilinx},
  title  = {{V}itis-{AI}},
  url    = {https://github.com/Xilinx/Vitis-AI},
  year   = 2022
}

@inproceedings{wangPeekingCurtainsServerlessb,
  author    = {Liang Wang and Mengyuan Li and Yinqian Zhang and Thomas Ristenpart and Michael Swift},
  title     = {Peeking Behind the Curtains of Serverless Platforms},
  booktitle = {2018 USENIX Annual Technical Conference (USENIX ATC 18)},
  year      = {2018},
  isbn      = {ISBN 978-1-939133-01-4},
  address   = {Boston, MA},
  pages     = {133--146},
  url       = {https://www.usenix.org/conference/atc18/presentation/wang-liang},
  publisher = {USENIX Association},
  month     = jul,
  abstract  = {Serverless computing is an emerging paradigm in which an application's resource provisioning and scaling are managed by third-party services. Examples include AWS Lambda, Azure Functions, and Google Cloud Functions. Behind these services' easy-to-use APIs are opaque, complex infrastructure and management ecosystems. Taking on the viewpoint of a serverless customer, we conduct the largest measurement study to date, launching more than 50,000 function instances across these three services, in order to characterize their architectures, performance, and resource management efficiency. We explain how the platforms isolate the functions of different accounts, using either virtual machines or containers, which has important security implications. We characterize performance in terms of scalability, coldstart latency, and resource efficiency, with highlights including that AWS Lambda adopts a bin-packing-like strategy to maximize VM memory utilization, that severe contention between functions can arise in AWS and Azure, and that Google had bugs that allow customers to use resources for free.},
  langid    = {english}
}
@inproceedings{yalles2022riscless,
  author    = {Yalles, SidAhmed and Handaoui, Mohamed and Dartois, Jean-Emile and Barais, Olivier and d’Orazio, Laurent and Boukhobza, Jalil},
  booktitle = {2022 30th Euromicro International Conference on Parallel, Distributed and Network-based Processing (PDP)},
  title     = {{RISCLESS}: {A} Reinforcement Learning Strategy to Guarantee {SLA} on Cloud Ephemeral and Stable Resources},
  year      = {2022},
  volume    = {},
  number    = {},
  pages     = {83-87},
  doi       = {10.1109/PDP55904.2022.00021}
}
@inproceedings{yangINFlessNativeServerless2022,
  title      = {{INFless}: {A} Native Serverless System for Low-Latency, High-Throughput Inference},
  shorttitle = {{INFless}},
  booktitle  = {Proceedings of the 27th {{ACM International Conference}} on {{Architectural Support}} for {{Programming Languages}} and {{Operating Systems}}},
  author     = {Yang, Yanan and Zhao, Laiping and Li, Yiming and Zhang, Huanyu and Li, Jie and Zhao, Mingyang and Chen, Xingzhen and Li, Keqiu},
  year       = {2022},
  month      = feb,
  pages      = {768--781},
  publisher  = {{ACM}},
  address    = {{Lausanne Switzerland}},
  url        = {https://doi.org/10.1145/3503222.3507709},
  doi        = {10.1145/3503222.3507709},
  abstract   = {Modern websites increasingly rely on machine learning (ML) to improve their business efficiency. Developing and maintaining ML services incurs high costs for developers. Although serverless systems are a promising solution to reduce costs, we find that the current general purpose serverless systems cannot meet the low latency, high throughput demands of ML services.},
  isbn       = {978-1-4503-9205-1},
  langid     = {english}
}
@inproceedings{zhangMArkExploitingCloud,
  author    = {Chengliang Zhang and Minchen Yu and Wei Wang and Feng Yan},
  title     = {{MArk}: {E}xploiting Cloud Services for Cost-Effective, {SLO}-Aware Machine Learning Inference Serving},
  booktitle = {2019 USENIX Annual Technical Conference (USENIX ATC 19)},
  year      = {2019},
  isbn      = {978-1-939133-03-8},
  address   = {Renton, WA},
  pages     = {1049--1062},
  url       = {https://www.usenix.org/conference/atc19/presentation/zhang-chengliang},
  publisher = {USENIX Association},
  month     = jul,
  abstract  = {The advances of Machine Learning (ML) have sparked a growing demand of ML-as-a-Service: developers train ML models and publish them in the cloud as online services to provide low-latency inference at scale. The key challenge of ML model serving is to meet the response-time Service-Level Objectives (SLOs) of inference workloads while minimizing the serving cost. In this paper, we tackle the dual challenge of SLO compliance and cost effectiveness with MArk (Model Ark), a general-purpose inference serving system built in Amazon Web Services (AWS). MArk employs three design choices tailor-made for inference workload. First, MArk dynamically batches requests and opportunistically serves them using expensive hardware accelerators (e.g., GPU) for improved performance-cost ratio. Second, instead of relying on feedback control scaling or over-provisioning to serve dynamic workload, which can be too slow or too expensive for inference serving, MArk employs predictive autoscaling to hide the provisioning latency at low cost. Third, given the stateless nature of inference serving, MArk exploits the flexible, yet costly serverless instances to cover the occasional load spikes that are hard to predict. We evaluated the performance of MArk using several state-of-the-art ML models trained in popular frameworks including TensorFlow, MXNet, and Keras. Compared with the premier industrial ML serving platform SageMaker, MArk reduces the serving cost up to 7.8\texttimes{} while achieving even better latency performance.},
  langid    = {english}
}

%% Ajouts

@article{westerlundEmergenceDeepfakeTechnology2019,
  title = {The {{Emergence}} of {{Deepfake Technology}}: {{A Review}}},
  shorttitle = {The {{Emergence}} of {{Deepfake Technology}}},
  author = {Westerlund, Mika},
  year = {2019},
  month = jan,
  journal = {Technology Innovation Management Review},
  volume = {9},
  number = {11},
  pages = {39--52},
  issn = {19270321},
  doi = {10.22215/timreview/1282},
  urldate = {2024-07-18},
  langid = {english},
}