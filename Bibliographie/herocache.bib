@inproceedings{slimani:hal-04159551,
  title        = {{Characterizing Intrusion Detection Systems On Heterogeneous Embedded Platforms}},
  author       = {Slimani, Cam{\'e}lia and Morge-Rollet, Louis and Lemarchand, Laurent and Espes, David and Le Roy, Fr{\'e}d{\'e}ric and Boukhobza, Jalil},
  year         = 2023,
  month        = Sep,
  booktitle    = {{DSD '23}},
  address      = {Durres, Albania},
  keywords     = {Intrusion Detection Systems ; IDS ; swarms of drones ; Security/Latency/Energy trade-off ; machine learning},
  hal_id       = {hal-04159551},
  hal_version  = {v1}
}

@article{SLIMANI2024,
title = {{A study on characterizing energy, latency and security for Intrusion Detection Systems on heterogeneous embedded platforms}},
journal = {{Future Generation Computer Systems}},
year = {2024},
issn = {0167-739X},
doi = {10.1016/j.future.2024.07.051},
url = {https://doi.org/10.1016/j.future.2024.07.051},
author = {Camélia Slimani and Louis Morge-Rollet and Laurent Lemarchand and David Espes and Frédéric {Le Roy} and Jalil Boukhobza},
keywords = {Intrusion Detection Systems, Swarms of drones, Security/latency/energy trade-off, Machine learning},
}

@inproceedings{herofake,
  title        = {{HeROfake: Heterogeneous Resources Orchestration in a Serverless Cloud – An Application to Deepfake Detection}},
  author       = {Lannurien, Vincent and D'Orazio, Laurent and Barais, Olivier and Bernard, Esther and Weppe, Olivier and Beaulieu, Laurent and Kacete, Amine and Paquelet, Stéphane and Boukhobza, Jalil},
  year         = 2023,
  booktitle    = {{CCGrid '23}},
  volume       = {},
  number       = {},
  pages        = {154--165},
  doi          = {10.1109/CCGrid57682.2023.00024}
}
@inbook{Lannurien2023,
  title        = {{Serverless Cloud Computing: State of the Art and Challenges}},
  author       = {Lannurien, Vincent and D'Orazio, Laurent and Barais, Olivier and Boukhobza, Jalil},
  year         = 2023,
  booktitle    = {Serverless Computing: Principles and Paradigms},
  publisher    = {Springer International Publishing},
  address      = {Cham},
  pages        = {275--316},
  doi          = {10.1007/978-3-031-26633-1_11},
  isbn         = {978-3-031-26633-1},
  editor       = {Krishnamurthi, Rajalakshmi and Kumar, Adarsh and Gill, Sukhpal Singh and Buyya, Rajkumar},
  abstract     = {The serverless model represents a paradigm shift in the cloud: as opposed to traditional cloud computing service models, serverless customers do not reserve hardware resources. The execution of their code is event-driven (HTTP requests, cron jobs, etc.) and billing is based on actual resource usage. In return, the responsibility of resource allocation and task placement lies on the provider. While serverless in the wild is mainly advertised as a public cloud offering, solutions are actively developed and backed by solid actors in the industry to allow the development of private cloud serverless platforms. The first generation of serverless offers, ``Function as a Service'' (FaaS), has severe shortcomings that can offset the potential benefits for both customers and providers---in terms of spendings and reliability on the customer side, and in terms of resources multiplexing on the provider side. Circumventing these flaws would allow considerable savings in money and energy for both providers and tenants. This chapter aims at establishing a comprehensive tour of these limitations, and presenting state-of-the-art studies to mitigate weaknesses that are currently holding serverless back from becoming the de facto cloud computing model. The main challenges related to the deployment of such a cloud platform are discussed and some perspectives for future directions in research are given.}
}
@inproceedings{yanHermesEfficientCache2020,
  title        = {Hermes: {{Efficient Cache Management}} for {{Container-based Serverless Computing}}},
  shorttitle   = {Hermes},
  author       = {Yan, Bowen and Gao, Heran and Wu, Heng and Zhang, Wenbo and Hua, Lei and Huang, Tao},
  year         = 2020,
  month        = nov,
  booktitle    = {{Internetware '20}},
  publisher    = {{ACM}},
  address      = {{Singapore Singapore}},
  pages        = {136--145},
  doi          = {10.1145/3457913.3457925},
  isbn         = {978-1-4503-8819-1},
  urldate      = {2023-03-06},
  abstract     = {Serverless computing systems are shifting towards shorter function durations and larger degrees of parallelism to eliminate intolerable latency. For container-based serverless computing, the state-of-theart efforts fail to ensure low latency because on-demand container images reloading from remote storage can increase the data transmission rate and downgrades system performance. In this paper we propose Hermes with a two-level caching mechanism to reduce the latency and minimize data transmission rate when massive serverless workloads arrive. Hermes optimizes memory caching by persisting metadata cache and prolonging the lifetime of file cache to improve the cache efficiency of image files. Instead of reclaiming memory, Hermes uses disk caching to reduce memory usage, and gets a low data transmission rate by reloading from local disk cache. Experiment results show that Hermes can reduce 90\% of the data transmission rate and improve the runtime performance of serverless workloads up to 5\texttimes{} in a machine with 300 concurrent containers compared to state-of-the-art efforts.},
  langid       = {english}
}
@inproceedings{wawrzoniakBoxerDataAnalytics2021a,
  title        = {Boxer: {{Data Analytics}} on {{Network-enabled Serverless Platforms}}},
  shorttitle   = {Boxer},
  author       = {Wawrzoniak, Mike and M{\"u}ller, Ingo and Fraga Barcelos Paulus Bruno, Rodrigo and Alonso, Gustavo},
  year         = 2021,
  month        = jan,
  booktitle    = {{CIDR 2021}},
  publisher    = {www.cidrdb.org},
  doi          = {10.3929/ETHZ-B-000456492},
  urldate      = {2023-05-24},
  copyright    = {Creative Commons Attribution 3.0 Unported, info:eu-repo/semantics/openAccess},
  abstract     = {Serverless is an attractive platform for a variety of applications in the cloud due to its promise of elasticity, low cost, and fast deployment. Instead of using traditional virtual machine services and a fixed infrastructure, which incurs considerable costs to operate and run, Function-as-a-Service allows triggering short computations on demand with the cost proportional to the time the functions are running. As appealing as the idea is, recent work has shown that for data processing applications (regardless of whether it is OLTP, OLAP, or ML) existing serverless platforms are inadequate and additional services are needed in practice, often to address the lack of communication capabilities between functions. In this paper, we demonstrate how to enable function-to-function communication using conventional TCP/IP and show how the ability to communicate can be used to implement data processing on serverless platforms in a more efficient manner than it was possible until now. Our benchmarks show a speedup as high as 11 \texttimes{} in TPC-H queries over systems that use cloud storage to communicate across functions, sustained function-to-function throughput of 621 Mbit/s, and a round-trip latency of less than 1 ms.},
  langid       = {english}
}
@inproceedings{shahradServerlessWildCharacterizing,
  title        = {{Serverless in the Wild: Characterizing and Optimizing the Serverless Workload at a Large Cloud Provider}},
  author       = {Shahrad, Mohammad and Fonseca, Rodrigo and Goiri, {\'I}{\~n}igo and Chaudhry, Gohar and Batum, Paul and Cooke, Jason and Laureano, Eduardo and Tresness, Colby and Russinovich, Mark and Bianchini, Ricardo},
  year         = 2020,
  booktitle    = {{USENIX ATC'20}},
  publisher    = {USENIX Association},
  address      = {USA},
  pages        = 14,
  isbn         = {978-1-939133-14-4},
  abstract     = {Function as a Service (FaaS) has been gaining popularity as a way to deploy computations to serverless backends in the cloud. This paradigm shifts the complexity of allocating and provisioning resources to the cloud provider, which has to provide the illusion of always-available resources (i.e., fast function invocations without cold starts) at the lowest possible resource cost. Doing so requires the provider to deeply understand the characteristics of the FaaS workload. Unfortunately, there has been little to no public information on these characteristics. Thus, in this paper, we first characterize the entire production FaaS workload of Azure Functions. We show for example that most functions are invoked very infrequently, but there is an 8-order-of-magnitude range of invocation frequencies. Using observations from our characterization, we then propose a practical resource management policy that significantly reduces the number of function cold starts, while spending fewer resources than state-of-the-practice policies.},
  langid       = {english},
  articleno    = 14,
  numpages     = 14
}
@inproceedings{mullerLambadaInteractiveData2020,
  title        = {{Lambada: Interactive Data Analytics on Cold Data Using Serverless Cloud Infrastructure}},
  shorttitle   = {{L}ambada},
  author       = {M{\"u}ller, Ingo and Marroqu{\'i}n, Renato and Alonso, Gustavo},
  year         = 2020,
  month        = jun,
  booktitle    = {{SIGMOD 2020}},
  publisher    = {{ACM}},
  address      = {{Portland OR USA}},
  pages        = {115--130},
  doi          = {10.1145/3318464.3389758},
  isbn         = {978-1-4503-6735-6},
  abstract     = {Serverless computing has recently attracted a lot of attention from research and industry due to its promise of ultimate elasticity and operational simplicity. However, there is no consensus yet on whether or not the approach is suitable for data processing. In this paper, we present Lambada, a serverless distributed data processing framework designed to explore how to perform data analytics on serverless computing. In our analysis, supported with extensive experiments, we show in which scenarios serverless makes sense from an economic and performance perspective. We address several important technical questions that need to be solved to support data analytics and present examples from several domains where serverless offers a cost and performance advantage over existing solutions.},
  langid       = {english}
}
@inproceedings{mahgoubORIONThreeRights,
  title        = {{ORION and the Three Rights: Sizing, Bundling, and Prewarming for Serverless DAGs}},
  author       = {Ashraf Mahgoub and Edgardo Barsallo Yi and Karthick Shankar and Sameh Elnikety and Somali Chaterji and Saurabh Bagchi},
  year         = 2022,
  month        = jul,
  booktitle    = {{OSDI 22}},
  publisher    = {USENIX Association},
  address      = {Carlsbad, CA},
  pages        = {303--320},
  isbn         = {978-1-939133-28-1}
}
@article{burckhardtNetheriteEfficientExecution,
  title        = {{Netherite: Efficient Execution of Serverless Workflows}},
  author       = {Burckhardt, Sebastian and Chandramouli, Badrish and Gillum, Chris and Justo, David and Kallas, Konstantinos and McMahon, Connor and Meiklejohn, Christopher S. and Zhu, Xiangfeng},
  year         = 2022,
  month        = {apr},
  journal      = {Proc. VLDB Endow.},
  publisher    = {VLDB Endowment},
  volume       = 15,
  number       = 8,
  pages        = {1591–1604},
  doi          = {10.14778/3529337.3529344},
  issn         = {2150-8097},
  issue_date   = {April 2022},
  abstract     = {Serverless is a popular choice for cloud service architects because it can provide scalability and load-based billing with minimal developer effort. Functions-as-a-service (FaaS) are originally stateless, but emerging frameworks add stateful abstractions. For instance, the widely used Durable Functions (DF) allow developers to write advanced serverless applications, including reliable workflows and actors, in a programming language of choice. DF implicitly and continuosly persists the state and progress of applications, which greatly simplifies development, but can create an IOps bottleneck.To improve efficiency, we introduce Netherite, a novel architecture for executing serverless workflows on an elastic cluster. Netherite groups the numerous application objects into a smaller number of partitions, and pipelines the state persistence of each partition. This improves latency and throughput, as it enables workflow steps to group commit, even if causally dependent. Moreover, Netherite leverages FASTER's hybrid log approach to support larger-than-memory application state, and to enable efficient partition movement between compute hosts.Our evaluation shows that (a) Netherite achieves lower latency and higher throughput than the original DF engine, by more than an order of magnitude in some cases, and (b) that Netherite has lower latency than some commonly used alternatives, like AWS Step Functions or cloud storage triggers.},
  numpages     = 14
}
@inproceedings{mohanAgileColdStartsa,
  title        = {{Agile Cold Starts for Scalable Serverless}},
  author       = {Anup Mohan and Harshad Sane and Kshitij Doshi and Saikrishna Edupuganti and Naren Nayak and Vadim Sukhomlinov},
  year         = 2019,
  month        = jul,
  booktitle    = {{HotCloud 19}},
  publisher    = {USENIX Association},
  address      = {Renton, WA},
  pages        = 6,
  abstract     = {The Serverless or Function-as-a-Service (FaaS) model capitalizes on lightweight execution by packaging code and dependencies together for just-in-time dispatch. Often a container environment has to be set up afresh\textendash{} a condition called ``cold start", and in such cases, performance suffers and overheads mount, both deteriorating rapidly under high concurrency. Caching and reusing previously employed containers ties up memory and risks information leakage. Latency for cold starts is frequently due to work and wait-times in setting up various dependencies \textendash{} such as in initializing networking elements. This paper proposes a solution that pre-crafts such resources and then dynamically reassociates them with baseline containers. Applied to networking, this approach demonstrates an order of magnitude gain in cold starts, negligible memory consumption, and flat startup time under rising concurrency.},
  langid       = {english}
}
@inproceedings{vahidiniaColdStartServerless2020,
  title        = {{Cold Start in Serverless Computing: Current Trends and Mitigation Strategies}},
  shorttitle   = {Cold {{Start}} in {{Serverless Computing}}},
  author       = {Vahidinia, Parichehr and Farahani, Bahar and Aliee, Fereidoon Shams},
  year         = 2020,
  month        = aug,
  booktitle    = {{COINS 2020}},
  publisher    = {{IEEE}},
  address      = {{Barcelona, Spain}},
  pages        = {1--7},
  doi          = {10.1109/COINS49042.2020.9191377},
  isbn         = {978-1-72816-371-0},
  abstract     = {Serverless Computing is the latest cloud computing model, which facilitates application development. By adopting and leveraging the modern paradigm of Serverless Computing, developers do not need to manage the servers. In this computational model, the executables are independent functions that are individually deployed on a Serverless platform offering instant per-request elasticity. Such elasticity typically comes at the cost of the "Cold Starts" problem. This phenomenon is associated with a delay occurring due to provision a runtime container to execute the functions. Shortly after Amazon introduced this computing model with the AWS Lambda platform in 2014, several open source and commercial platforms also started embracing and offering this technology. Each platform has its own solution to deal with Cold Starts. The evaluation of the performance of each platform under the load and factors influencing the cold start problem has received much attention over the past few years. This paper provides a comprehensive overview on the recent advancements and stateof-the-art works in mitigating the cold start delay. Moreover, several sets of experiments have been performed to study the behavior of the AWS Lambda as the base platform with respect to the cold start delay.},
  langid       = {english}
}
@inproceedings{sureshENSUREEfficientScheduling2020,
  title        = {{ENSURE: Efficient Scheduling and Autonomous Resource Management in Serverless Environments}},
  shorttitle   = {{ENSURE}},
  author       = {Suresh, Amoghavarsha and Somashekar, Gagan and Varadarajan, Anandh and Kakarla, Veerendra Ramesh and Upadhyay, Hima and Gandhi, Anshul},
  year         = 2020,
  month        = aug,
  booktitle    = {{ACSOS 2020}},
  publisher    = {{IEEE}},
  address      = {{Washington, DC, USA}},
  pages        = {1--10},
  doi          = {10.1109/ACSOS49614.2020.00020},
  isbn         = {978-1-72817-277-4},
  abstract     = {An imminent challenge in the serverless computing landscape is the escalating cost of infrastructure needed to handle the growing traffic at scale. This work presents ENSURE, a function-level scheduler and autonomous resource manager designed to minimize provider resource costs while meeting customer performance requirements. ENSURE works by classifying incoming function requests at runtime and carefully regulating the resource usage of colocated functions on each invoker. Beyond a single invoker, ENSURE elastically scales capacity, using concepts from operations research, in response to varying workload traffic to prevent cold starts. Finally, ENSURE schedules requests by concentrating load on an adequate number of invokers to encourage reuse of active hosts (thus further avoiding cold starts) and allow unneeded capacity to provably and gracefully time out. We implement ENSURE on Apache OpenWhisk and show that, across several serverless applications and compared to existing baselines, ENSURE significantly improves resource efficiency, by as much as 52\%, while providing acceptable application latency.},
  langid       = {english}
}
@inproceedings{wangPeekingCurtainsServerlessb,
  title        = {{Peeking Behind the Curtains of Serverless Platforms}},
  author       = {Liang Wang and Mengyuan Li and Yinqian Zhang and Thomas Ristenpart and Michael Swift},
  year         = 2018,
  month        = jul,
  booktitle    = {{USENIX ATC 18}},
  publisher    = {USENIX Association},
  address      = {Boston, MA},
  pages        = {133--146},
  isbn         = {ISBN 978-1-939133-01-4},
  abstract     = {Serverless computing is an emerging paradigm in which an application's resource provisioning and scaling are managed by third-party services. Examples include AWS Lambda, Azure Functions, and Google Cloud Functions. Behind these services' easy-to-use APIs are opaque, complex infrastructure and management ecosystems. Taking on the viewpoint of a serverless customer, we conduct the largest measurement study to date, launching more than 50,000 function instances across these three services, in order to characterize their architectures, performance, and resource management efficiency. We explain how the platforms isolate the functions of different accounts, using either virtual machines or containers, which has important security implications. We characterize performance in terms of scalability, coldstart latency, and resource efficiency, with highlights including that AWS Lambda adopts a bin-packing-like strategy to maximize VM memory utilization, that severe contention between functions can arise in AWS and Azure, and that Google had bugs that allow customers to use resources for free.},
  langid       = {english}
}
@article{jeon2021run,
  title        = {{Run your visual-inertial odometry on NVIDIA Jetson: Benchmark tests on a micro aerial vehicle}},
  author       = {Jeon, Jinwoo and Jung, Sungwook and Lee, Eungchang and Choi, Duckyu and Myung, Hyun},
  year         = 2021,
  journal      = {IEEE Robotics and Automation Letters},
  publisher    = {IEEE},
  volume       = 6,
  number       = 3,
  pages        = {5332--5339}
}
@article{sousa2019platform,
  title        = {A platform of unmanned surface vehicle swarms for real time monitoring in aquaculture environments},
  author       = {Sousa, Daniela and Hernandez, Diego and Oliveira, Francisco and Lu{\'\i}s, Miguel and Sargento, Susana},
  year         = 2019,
  journal      = {Sensors},
  publisher    = {MDPI},
  volume       = 19,
  number       = 21,
  pages        = 4695
}
@inproceedings{moustafa2015unsw,
  title        = {{UNSW-NB15: a comprehensive data set for network intrusion detection systems (UNSW-NB15 network data set)}},
  author       = {Moustafa, Nour and Slay, Jill},
  year         = 2015,
  booktitle    = {2015 military communications and information systems conference (MilCIS)},
  pages        = {1--6},
  organization = {IEEE}
}
@article{8570043,
  title        = {{Ultra-Reliable IoT Communications with UAVs: A Swarm Use Case}},
  author       = {Yuan, Zhenhui and Jin, Jie and Sun, Lingling and Chin, Kwan-Wu and Muntean, Gabriel-Miro},
  year         = 2018,
  journal      = {IEEE Communications Magazine},
  volume       = 56,
  number       = 12,
  pages        = {90--96},
  doi          = {10.1109/MCOM.2018.1800161}
}
@article{9437802,
  title        = {{Efficient and Secured Swarm Pattern Multi-UAV Communication}},
  author       = {Raja, Gunasekaran and Anbalagan, Sudha and Ganapathisubramaniyan, Aishwarya and Selvakumar, Madhumitha Sri and Bashir, Ali Kashif and Mumtaz, Shahid},
  year         = 2021,
  journal      = {IEEE Transactions on Vehicular Technology},
  volume       = 70,
  number       = 7,
  pages        = {7050--7058},
  doi          = {10.1109/TVT.2021.3082308}
}
@article{ZHOU2022100,
  title        = {{Improving multi-target cooperative tracking guidance for UAV swarms using multi-agent reinforcement learning}},
  author       = {Wenhong ZHOU and Jie LI and Zhihong LIU and Lincheng SHEN},
  year         = 2022,
  journal      = {Chinese Journal of Aeronautics},
  volume       = 35,
  number       = 7,
  pages        = {100--112},
  doi          = {https://doi.org/10.1016/j.cja.2021.09.008},
  issn         = {1000-9361},
  keywords     = {Decentralized cooperation, Maximum reciprocal reward, Multi-agent actor-critic, Pointwise mutual information, Reinforcement learning},
  abstract     = {Multi-Target Tracking Guidance (MTTG) in unknown environments has great potential values in applications for Unmanned Aerial Vehicle (UAV) swarms. Although Multi-Agent Deep Reinforcement Learning (MADRL) is a promising technique for learning cooperation, most of the existing methods cannot scale well to decentralized UAV swarms due to their computational complexity or global information requirement. This paper proposes a decentralized MADRL method using the maximum reciprocal reward to learn cooperative tracking policies for UAV swarms. This method reshapes each UAV's reward with a regularization term that is defined as the dot product of the reward vector of all neighbor UAVs and the corresponding dependency vector between the UAV and the neighbors. And the dependence between UAVs can be directly captured by the Pointwise Mutual Information (PMI) neural network without complicated aggregation statistics. Then, the experience sharing Reciprocal Reward Multi-Agent Actor-Critic (MAAC-R) algorithm is proposed to learn the cooperative sharing policy for all homogeneous UAVs. Experiments demonstrate that the proposed algorithm can improve the UAVs’ cooperation more effectively than the baseline algorithms, and can stimulate a rich form of cooperative tracking behaviors of UAV swarms. Besides, the learned policy can better scale to other scenarios with more UAVs and targets.}
}
% TODO
@inproceedings{abdiPaletteLoadBalancing2023,
  title        = {{Palette Load Balancing: Locality Hints for Serverless Functions}},
  shorttitle   = {Palette {{Load Balancing}}},
  author       = {Abdi, Mania and Ginzburg, Samuel and Lin, Xiayue Charles and Faleiro, Jose and Chaudhry, Gohar Irfan and Goiri, Inigo and Bianchini, Ricardo and Berger, Daniel S and Fonseca, Rodrigo},
  year         = 2023,
  month        = may,
  booktitle    = {{EuroSys '23}},
  publisher    = {{ACM}},
  address      = {{Rome Italy}},
  pages        = {365--380},
  doi          = {10.1145/3552326.3567496},
  isbn         = {978-1-4503-9487-1},
  urldate      = {2023-05-15},
  abstract     = {Function-as-a-Service (FaaS) serverless computing enables a simple programming model with almost unbounded elasticity. Unfortunately, current FaaS platforms achieve this flexibility at the cost of lower performance for data-intensive applications compared to a serverful deployment. The ability to have computation close to data is a key missing feature. We introduce Palette load balancing, which offers FaaS applications a simple mechanism to express locality to the platform, through hints we term ``colors''. Palette maintains the serverless nature of the service \textendash{} users are still not allocating resources \textendash{} while allowing the platform to place successive invocations related to each other on the same executing node. We compare a prototype of the Palette load balancer to a state-of-the-art locality-oblivious load balancer on representative examples of three applications. For a serverless web application with a local cache, Palette improves the hit ratio by 6x. For a serverless version of Dask, Palette improves run times by 46\% and 40\% on Task Bench and TPC-H, respectively. On a serverless version of NumS, Palette improves run times by 37\%. These improvements largely bridge the gap to serverful implementation of the same systems.},
  langid       = {english}
}
@inproceedings{zijunFassflowEfficient2022,
  title        = {{FaaSFlow: Enable Efficient Workflow Execution for Function-as-a-Service}},
  author       = {Li, Zijun and Liu, Yushi and Guo, Linsong and Chen, Quan and Cheng, Jiagan and Zheng, Wenli and Guo, Minyi},
  year         = 2022,
  booktitle    = {{{ASPLOS '22}}},
  location     = {Lausanne, Switzerland},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  pages        = {782–796},
  doi          = {10.1145/3503222.3507717},
  isbn         = 9781450392051,
  abstract     = {Serverless computing (Function-as-a-Service) provides fine-grain resource sharing by running functions (or Lambdas) in containers. Data-dependent functions are required to be invoked following a pre-defined logic, which is known as serverless workflows. However, our investigation shows that the traditional master-worker based workflow execution architecture performs poorly in serverless context. One significant overhead results from the master-side workflow schedule pattern, with which the functions are triggered in the master node and assigned to worker nodes for execution. Besides, the data movement between workers also reduces the throughput. To this end, we present a worker-side workflow schedule pattern for serverless workflow execution. Following the design, we implement FaaSFlow to enable efficient workflow execution in the serverless context. Besides, we propose an adaptive storage library FaaStore that enables fast data transfer between functions on the same node without through the database. Experiment results show that FaaSFlow effectively mitigates the workflow scheduling overhead by 74.6\% on average and data transmission overhead by 95\% at most. When the network bandwidth fluctuates, FaaSFlow-FaaStore reduces the throughput degradation by 23.0\%, and is able to multiply the utilization of network bandwidth by 1.5X-4X.},
  numpages     = 15,
  keywords     = {graph partition, master-worker, FaaS, serverless workflows}
}
@inproceedings{bhasiCypressInputSizesensitive2022,
  title        = {{Cypress: Input Size-Sensitive Container Provisioning and Request Scheduling for Serverless Platforms}},
  shorttitle   = {Cypress},
  author       = {Bhasi, Vivek M. and Gunasekaran, Jashwant Raj and Sharma, Aakash and Kandemir, Mahmut Taylan and Das, Chita},
  year         = 2022,
  month        = nov,
  booktitle    = {{SoCC '22}},
  publisher    = {{ACM}},
  address      = {{San Francisco California}},
  pages        = {257--272},
  doi          = {10.1145/3542929.3563464},
  isbn         = {978-1-4503-9414-7},
  urldate      = {2023-03-08},
  abstract     = {The growing popularity of the serverless platform has seen an increase in the number and variety of applications (apps) being deployed on it. The majority of these apps process user-provided input to produce the desired results. Existing work in the area of input-sensitive profiling has empirically shown that many such apps have input size\textendash dependent execution times which can be determined through modelling techniques. Nevertheless, existing serverless resource management frameworks are agnostic to the input size\textendash sensitive nature of these apps. We demonstrate in this paper that this can potentially lead to container over-provisioning and/or end-to-end Service Level Objective (SLO) violations. To address this, we propose Cypress, an input size\textendash sensitive resource management framework, that minimizes the containers provisioned for apps, while ensuring a high degree of SLO compliance. We perform an extensive evaluation of Cypress on top of a Kubernetes-managed cluster using 5 apps from the AWS Serverless Application Repository and/or OpenFaaS Function Store with real-world traces and varied input size distributions. Our experimental results show that Cypress spawns up to 66\% fewer containers, thereby, improving container utilization and saving cluster-wide energy by up to 2.95\texttimes{} and 23\%, respectively, versus state-of-the-art frameworks, while remaining highly SLO-compliant (up to 99.99\%). CCS Concepts \textbullet{} Computer systems organization \textrightarrow{} Cloud Computing; Resource-Management; Scheduling.},
  langid       = {english}
}
@inproceedings{zhangFIRSTExploitingMultiDimensional2023,
  title        = {{FIRST: Exploiting the Multi-Dimensional Attributes of Functions for Power-Aware Serverless Computing}},
  shorttitle   = {{{FIRST}}},
  author       = {Zhang, Lu and Li, Chao and Wang, Xinkai and Feng, Weiqi and Yu, Zheng and Chen, Quan and Leng, Jingwen and Guo, Minyi and Yang, Pu and Yue, Shang},
  year         = 2023,
  month        = may,
  booktitle    = {{IPDPS 2023}},
  publisher    = {{IEEE}},
  address      = {{St. Petersburg, FL, USA}},
  pages        = {864--874},
  doi          = {10.1109/IPDPS54959.2023.00091},
  isbn         = 9798350337662,
  urldate      = {2023-12-08},
  abstract     = {Emerging cloud-native development models raise new challenges for managing server performance and power at microsecond scale. Compared with traditional cloud workloads, serverless functions exhibit unprecedented heterogeneity, variability, and dynamicity. Designing cloud-native power management schemes for serverless functions requires significant engineering effort. Current solutions remain sub-optimal since their orchestration process is often one-sided, lacking a systematic view. A key obstacle to truly efficient function deployment is the fundamental wide abstraction gap between the upper-layer request scheduling and the low-level hardware execution.},
  langid       = {english}
}
@inproceedings{smithFaDOFaaSFunctions2022,
  title        = {{FaDO: FaaS Functions and Data Orchestrator for Multiple Serverless Edge-Cloud Clusters}},
  shorttitle   = {{{FaDO}}},
  author       = {Smith, Christopher Peter and Jindal, Anshul and Chadha, Mohak and Gerndt, Michael and Benedict, Shajulin},
  year         = 2022,
  month        = may,
  booktitle    = {{ICFEC 2022}},
  publisher    = {{IEEE}},
  address      = {{Messina, Italy}},
  pages        = {17--25},
  doi          = {10.1109/ICFEC54809.2022.00010},
  isbn         = {978-1-66549-524-0},
  urldate      = {2023-12-11},
  abstract     = {Function-as-a-Service (FaaS) is an attractive cloud computing model that simplifies application development and deployment. However, current serverless compute platforms do not consider data placement when scheduling functions. With the growing demand for edge-cloud continuum, multi-cloud, and multi-serverless applications, this flaw means serverless technologies are still ill-suited to latency-sensitive operations like media streaming. This work proposes a solution by presenting a tool called FaDO: FaaS Functions and Data Orchestrator, designed to allow data-aware functions scheduling across multiserverless compute clusters present at different locations, such as at the edge and in the cloud. FaDO works through headerbased HTTP reverse proxying and uses three load-balancing algorithms: 1) The Least Connections, 2) Round Robin, and 3) Random for load balancing the invocations of the function across the suitable serverless compute clusters based on the set storage policies. FaDO further provides users with an abstraction of the serverless compute cluster's storage, allowing users to interact with data across different storage services through a unified interface. In addition, users can configure automatic and policy-aware granular data replications, causing FaDO to spread data across the clusters while respecting location constraints. Load testing results show that it is capable of load balancing high-throughput workloads, placing functions near their data without contributing any significant performance overhead.},
  langid       = {english}
}
@inproceedings{fuerstIluvatarFastControl2023,
  title        = {{Il\'{u}vatar: A Fast Control Plane for Serverless Computing}},
  author       = {Fuerst, Alexander and Rehman, Abdul and Sharma, Prateek},
  year         = 2023,
  booktitle    = {{HPDC '23}},
  location     = {Orlando, FL, USA},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  pages        = {267–280},
  doi          = {10.1145/3588195.3592995},
  isbn         = 9798400701559,
  abstract     = {Providing efficient Functions as a Service (FaaS) is challenging due to the serverless programming model and highly heterogeneous and dynamic workloads. Great strides have been made in optimizing FaaS performance through scheduling, caching, virtualization, and other resource management techniques. The combination of these advances and growing FaaS workloads have pushed the performance bottleneck into the control plane itself. Current FaaS control planes like OpenWhisk introduce 100s of milliseconds of latency overhead, and are becoming unsuitable for high performance FaaS research and deployments.We present the design and implementation of Il\'{u}vatar, a fast, modular, extensible FaaS control plane which reduces the latency overhead by more than two orders of magnitude. Il\'{u}vatar has a worker-centric architecture and introduces a new function queue technique for managing function scheduling and overcommitment. Il\'{u}vatar is implemented in Rust in about 13,000 lines of code, and introduces only 3ms of latency overhead under a wide range of loads, which is more than 2 orders of magnitude lower than OpenWhisk.},
  numpages     = 14,
  keywords     = {open source, functions as a service, cloud computing, serverless computing}
}
@online{openwhisk,
  title        = {{OpenWhisk}},
  author       = {Apache},
  year         = 2022,
  url          = {https://openwhisk.apache.org/}
}
@online{knative,
  title        = {Knative},
  author       = {Cloud Native Computing Foundation},
  year         = 2022,
  url          = {https://knative.dev/}
}
@online{knative-autoscaling,
  title        = {{K}native -- {A}utoscaling},
  author       = {The {K}native Authors},
  year         = 2022,
  url          = {https://github.com/knative/serving/tree/main/docs/scaling}
}
@online{knative-concurrency,
  title        = {{K}native -- {C}onfiguraing concurrency},
  author       = {The {K}native Authors},
  year         = 2022,
  url          = {https://knative.dev/docs/serving/autoscaling/concurrency/#soft-versus-hard-concurrency-limits}
}
@online{simpy,
  title        = {{SimPy}},
  author       = {Team {SimPy}},
  year         = 2022,
  url          = {https://simpy.readthedocs.io/}
}
@article{ei-white-paper,
  title        = {{Edge Intelligence: Paving the Last Mile of Artificial Intelligence With Edge Computing}},
  author       = {Zhou, Zhi and Chen, Xu and Li, En and Zeng, Liekang and Luo, Ke and Zhang, Junshan},
  year         = 2019,
  journal      = {Proceedings of the IEEE},
  volume       = 107,
  number       = 8,
  pages        = {1738--1762},
  doi          = {10.1109/JPROC.2019.2918951}
}
@inproceedings{baller2021deepedgebench,
  title        = {{DeepEdgeBench: Benchmarking deep neural networks on edge devices}},
  author       = {Baller, Stephan Patrick and Jindal, Anshul and Chadha, Mohak and Gerndt, Michael},
  year         = 2021,
  booktitle    = {{IC2E 2021}},
  pages        = {20--30},
  organization = {IEEE}
}
@inproceedings{kljucaric2020,
  title        = {{Architectural Analysis of Deep Learning on Edge Accelerators}},
  author       = {Kljucaric, Luke and Johnson, Alex and George, Alan D.},
  year         = 2020,
  booktitle    = {{HPEC 2020}},
  volume       = {},
  number       = {},
  pages        = {1--7},
  doi          = {10.1109/HPEC43674.2020.9286209}
}
@article{eskandari2020,
  title        = {{Passban IDS: An Intelligent Anomaly-Based Intrusion Detection System for IoT Edge Devices}},
  author       = {Eskandari, Mojtaba and Janjua, Zaffar Haider and Vecchio, Massimo and Antonelli, Fabio},
  year         = 2020,
  journal      = {IEEE Internet of Things Journal},
  volume       = 7,
  number       = 8,
  pages        = {6882--6897},
  doi          = {10.1109/JIOT.2020.2970501}
}
@inproceedings{9928755,
  title        = {{On the Modelling and Analysis of Edge-Serverless Computing}},
  author       = {Li, Shuo and Baştuğ, Ejder and Di Renzo, Marco},
  year         = 2022,
  booktitle    = {{MeditCom 2022}},
  volume       = {},
  number       = {},
  pages        = {250--254},
  doi          = {10.1109/MeditCom55741.2022.9928755}
}
