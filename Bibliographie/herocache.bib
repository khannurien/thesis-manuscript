@inproceedings{slimani:hal-04159551,
  title = {Characterizing {{Intrusion Detection Systems On Heterogeneous Embedded Platforms}}},
  booktitle = {2023 26th {{Euromicro Conference}} on {{Digital System Design}} ({{DSD}})},
  author = {Slimani, Cam{\'e}lia and {Morge-Rollet}, Louis and Lemarchand, Laurent and Le Roy, Fr{\'e}d{\'e}ric and Espes, David and Boukhobza, Jalil},
  year = {2023},
  month = sep,
  pages = {278--285},
  publisher = {IEEE},
  address = {Golem, Albania},
  doi = {10.1109/DSD60849.2023.00047},
  abstract = {Swarms of drones are more and more used for critical missions and need to be protected against malicious users. Intrusion Detection Systems (IDS) are used to analyze network traffic in order to detect possible threats. Modern IDSs rely on machine learning models for such a sake. Because of the absence of central management in swarms of drones, IDSs constitute a good second-line protective measure. Investigating the execution of IDS (resource-hungry) algorithms on drone (resource-constrained) devices is crucial when it comes to optimizing energy, response time, memory footprint and algorithm precision. In addition, embedded platforms used in drones often incorporate heterogeneous computing platforms on which IDSs could be executed. In this paper, we present a methodology and results about characterizing the execution of different IDS models on various platform (CPUs, GPUs). In effect, as swarm of drones operate in different mission contexts (e.g. criticity level) and states (e.g. energy budget, memory footprint), it is important to explore which IDS model to run on which platforms for a given mission in a given context. For this sake, we evaluated several metrics on different platforms: energy and resource consumption, accuracy for malicious traffic detection and response time. The models tested (RF, CNN, DNN) have shown different performance according to the measured metrics and the chosen platform and proved to be relevant in different mission states.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {9798350344196},
  langid = {english},
}
@article{SLIMANI2024,
  title = {A Study on Characterizing Energy, Latency and Security for {{Intrusion Detection Systems}} on Heterogeneous Embedded Platforms},
  author = {Slimani, Cam{\'e}lia and {Morge-Rollet}, Louis and Lemarchand, Laurent and Espes, David and Le Roy, Fr{\'e}d{\'e}ric and Boukhobza, Jalil},
  year = {2025},
  month = jan,
  journal = {Future Generation Computer Systems},
  volume = {162},
  pages = {107473},
  issn = {0167739X},
  doi = {10.1016/j.future.2024.07.051},
  abstract = {Drone swarms are increasingly being used for critical missions and need to be protected against malicious users. Intrusion Detection Systems (IDS) are used to analyze network traffic in order to detect possible threats. Modern IDSs rely on machine learning models for this purpose. Optimizing the execution of resource-hungry IDS algorithms on resource-constrained drone devices, in terms of energy consumption, response time, memory footprint and guaranteed level of security, allows to extend the duration of missions. In addition, the embedded platforms used in drones often incorporate heterogeneous computing platforms on which IDSs could be executed. In this paper, we present a methodology and results about characterizing the execution of different IDS models on various processing elements, namely, Central Processing Units (CPU), Graphical Processing Units (GPU), Deep Learning Accelerators (DLA) and Field-Programmable Gate Array (FPGA). In effect, drones operate in different mission contexts in terms of criticality level, energy and memory budgets, and traffic load, so it is important to identify which IDS model to run on which processing element in a given context. For this sake, we evaluated several metrics on different platforms: energy and resource consumption, accuracy for malicious traffic detection and response time. Different models, namely Random Forests (RF), Convolutional Neural Networks (CNN) and Dense Neural Networks (DNN), have been implemented and characterized on different processing elements/platforms. This study has shown that relating the chosen implementation to the resources available on the drone is a judicious strategy to work on. It highlights the disparity between IDS implementations characteristics. For example, the inference time ranges from 1.27 {$\mu$}s to 30 ms, the energy consumption per inference is between 10.7 {$\mu$}J and 70 mJ, and the accuracy of the IDS models is between 65.73\% and 81.59\%. In addition, we develop a set of guidelines for choosing the best IDS model given a mission context.},
  langid = {english},
}
@inproceedings{herofake,
	title        = {{HeROfake: Heterogeneous Resources Orchestration in a Serverless Cloud – An Application to Deepfake Detection}},
	author       = {Lannurien, Vincent and D'Orazio, Laurent and Barais, Olivier and Bernard, Esther and Weppe, Olivier and Beaulieu, Laurent and Kacete, Amine and Paquelet, Stéphane and Boukhobza, Jalil},
	year         = 2023,
	booktitle    = {2023 IEEE/ACM 23rd International Symposium on Cluster, Cloud and Internet Computing (CCGrid)},
	volume       = {},
	number       = {},
	pages        = {154--165},
	doi          = {10.1109/CCGrid57682.2023.00024},
}
@inbook{Lannurien2023,
	title        = {{Serverless Cloud Computing: State of the Art and Challenges}},
	author       = {Lannurien, Vincent and D'Orazio, Laurent and Barais, Olivier and Boukhobza, Jalil},
	year         = 2023,
	booktitle    = {Serverless Computing: Principles and Paradigms},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {275--316},
	doi          = {10.1007/978-3-031-26633-1_11},
	isbn         = {978-3-031-26633-1},
	editor       = {Krishnamurthi, Rajalakshmi and Kumar, Adarsh and Gill, Sukhpal Singh and Buyya, Rajkumar},
	abstract     = {The serverless model represents a paradigm shift in the cloud: as opposed to traditional cloud computing service models, serverless customers do not reserve hardware resources. The execution of their code is event-driven (HTTP requests, cron jobs, etc.) and billing is based on actual resource usage. In return, the responsibility of resource allocation and task placement lies on the provider. While serverless in the wild is mainly advertised as a public cloud offering, solutions are actively developed and backed by solid actors in the industry to allow the development of private cloud serverless platforms. The first generation of serverless offers, ``Function as a Service'' (FaaS), has severe shortcomings that can offset the potential benefits for both customers and providers---in terms of spendings and reliability on the customer side, and in terms of resources multiplexing on the provider side. Circumventing these flaws would allow considerable savings in money and energy for both providers and tenants. This chapter aims at establishing a comprehensive tour of these limitations, and presenting state-of-the-art studies to mitigate weaknesses that are currently holding serverless back from becoming the de facto cloud computing model. The main challenges related to the deployment of such a cloud platform are discussed and some perspectives for future directions in research are given.}
}
@inproceedings{yanHermesEfficientCache2020,
	title        = {Hermes: {{Efficient Cache Management}} for {{Container-based Serverless Computing}}},
	shorttitle   = {Hermes},
	author       = {Yan, Bowen and Gao, Heran and Wu, Heng and Zhang, Wenbo and Hua, Lei and Huang, Tao},
	year         = 2020,
	month        = nov,
	booktitle    = {Proceedings of the 12th Asia-Pacific Symposium on Internetware},
    pages        = {136–145},
    numpages     = {10},
    keywords     = {cache management, container images, serverless computing},
    location     = {Singapore, Singapore},
    series       = {Internetware '20},
	publisher    = {Association for Computing Machinery},
    address      = {New York, NY, USA},
	doi          = {10.1145/3457913.3457925},
	isbn         = {978-1-4503-8819-1},
	abstract     = {Serverless computing systems are shifting towards shorter function durations and larger degrees of parallelism to eliminate intolerable latency. For container-based serverless computing, the state-of-theart efforts fail to ensure low latency because on-demand container images reloading from remote storage can increase the data transmission rate and downgrades system performance. In this paper we propose Hermes with a two-level caching mechanism to reduce the latency and minimize data transmission rate when massive serverless workloads arrive. Hermes optimizes memory caching by persisting metadata cache and prolonging the lifetime of file cache to improve the cache efficiency of image files. Instead of reclaiming memory, Hermes uses disk caching to reduce memory usage, and gets a low data transmission rate by reloading from local disk cache. Experiment results show that Hermes can reduce 90\% of the data transmission rate and improve the runtime performance of serverless workloads up to 5\texttimes{} in a machine with 300 concurrent containers compared to state-of-the-art efforts.},
	langid       = {english}
}
@inproceedings{wawrzoniakBoxerDataAnalytics2021a,
	title        = {Boxer: {{Data Analytics}} on {{Network-enabled Serverless Platforms}}},
	shorttitle   = {Boxer},
	author       = {Wawrzoniak, Michael and M{\"u}ller, Ingo and Fraga Barcelos Paulus Bruno, Rodrigo and Alonso, Gustavo},
	year         = 2021,
	month        = jan,
	booktitle    = {{CIDR 2021}},
	publisher    = {www.cidrdb.org},
	doi          = {10.3929/ETHZ-B-000456492},
	copyright    = {Creative Commons Attribution 3.0 Unported, info:eu-repo/semantics/openAccess},
	abstract     = {Serverless is an attractive platform for a variety of applications in the cloud due to its promise of elasticity, low cost, and fast deployment. Instead of using traditional virtual machine services and a fixed infrastructure, which incurs considerable costs to operate and run, Function-as-a-Service allows triggering short computations on demand with the cost proportional to the time the functions are running. As appealing as the idea is, recent work has shown that for data processing applications (regardless of whether it is OLTP, OLAP, or ML) existing serverless platforms are inadequate and additional services are needed in practice, often to address the lack of communication capabilities between functions. In this paper, we demonstrate how to enable function-to-function communication using conventional TCP/IP and show how the ability to communicate can be used to implement data processing on serverless platforms in a more efficient manner than it was possible until now. Our benchmarks show a speedup as high as 11 \texttimes{} in TPC-H queries over systems that use cloud storage to communicate across functions, sustained function-to-function throughput of 621 Mbit/s, and a round-trip latency of less than 1 ms.},
	langid       = {english}
}
@inproceedings{mahgoubORIONThreeRights,
	title        = {{ORION and the Three Rights: Sizing, Bundling, and Prewarming for Serverless DAGs}},
	author       = {Ashraf Mahgoub and Edgardo Barsallo Yi and Karthick Shankar and Sameh Elnikety and Somali Chaterji and Saurabh Bagchi},
    booktitle    = {16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
    year         = {2022},
    isbn         = {978-1-939133-28-1},
    address      = {Carlsbad, CA},
    pages        = {303--320},
    url          = {https://www.usenix.org/conference/osdi22/presentation/mahgoub},
    urldate      = {2024-09-18},
    publisher    = {USENIX Association},
    month        = jul
}
@article{jeon2021run,
	title        = {{Run your visual-inertial odometry on NVIDIA Jetson: Benchmark tests on a micro aerial vehicle}},
	author       = {Jeon, Jinwoo and Jung, Sungwook and Lee, Eungchang and Choi, Duckyu and Myung, Hyun},
	year         = 2021,
	journal      = {IEEE Robotics and Automation Letters},
	publisher    = {IEEE},
	volume       = 6,
	number       = 3,
	pages        = {5332--5339}
}
@article{sousa2019platform,
	title        = {A platform of unmanned surface vehicle swarms for real time monitoring in aquaculture environments},
	author       = {Sousa, Daniela and Hernandez, Diego and Oliveira, Francisco and Lu{\'\i}s, Miguel and Sargento, Susana},
	year         = 2019,
	journal      = {Sensors},
	publisher    = {MDPI},
	volume       = 19,
	number       = 21,
	pages        = 4695
}
@inproceedings{moustafa2015unsw,
	title        = {{UNSW-NB15: a comprehensive data set for network intrusion detection systems (UNSW-NB15 network data set)}},
	author       = {Moustafa, Nour and Slay, Jill},
	year         = 2015,
	booktitle    = {2015 military communications and information systems conference (MilCIS)},
	pages        = {1--6},
	organization = {IEEE}
}
@article{8570043,
	title        = {{Ultra-Reliable IoT Communications with UAVs: A Swarm Use Case}},
	author       = {Yuan, Zhenhui and Jin, Jie and Sun, Lingling and Chin, Kwan-Wu and Muntean, Gabriel-Miro},
	year         = 2018,
	journal      = {IEEE Communications Magazine},
	volume       = 56,
	number       = 12,
	pages        = {90--96},
	doi          = {10.1109/MCOM.2018.1800161}
}
@article{9437802,
	title        = {{Efficient and Secured Swarm Pattern Multi-UAV Communication}},
	author       = {Raja, Gunasekaran and Anbalagan, Sudha and Ganapathisubramaniyan, Aishwarya and Selvakumar, Madhumitha Sri and Bashir, Ali Kashif and Mumtaz, Shahid},
	year         = 2021,
	journal      = {IEEE Transactions on Vehicular Technology},
	volume       = 70,
	number       = 7,
	pages        = {7050--7058},
	doi          = {10.1109/TVT.2021.3082308}
}
@article{ZHOU2022100,
	title        = {{Improving multi-target cooperative tracking guidance for UAV swarms using multi-agent reinforcement learning}},
	author       = {Wenhong ZHOU and Jie LI and Zhihong LIU and Lincheng SHEN},
	year         = 2022,
	journal      = {Chinese Journal of Aeronautics},
	volume       = 35,
	number       = 7,
	pages        = {100--112},
	doi          = {10.1016/j.cja.2021.09.008},
	issn         = {1000-9361},
	keywords     = {Decentralized cooperation, Maximum reciprocal reward, Multi-agent actor-critic, Pointwise mutual information, Reinforcement learning},
	abstract     = {Multi-Target Tracking Guidance (MTTG) in unknown environments has great potential values in applications for Unmanned Aerial Vehicle (UAV) swarms. Although Multi-Agent Deep Reinforcement Learning (MADRL) is a promising technique for learning cooperation, most of the existing methods cannot scale well to decentralized UAV swarms due to their computational complexity or global information requirement. This paper proposes a decentralized MADRL method using the maximum reciprocal reward to learn cooperative tracking policies for UAV swarms. This method reshapes each UAV's reward with a regularization term that is defined as the dot product of the reward vector of all neighbor UAVs and the corresponding dependency vector between the UAV and the neighbors. And the dependence between UAVs can be directly captured by the Pointwise Mutual Information (PMI) neural network without complicated aggregation statistics. Then, the experience sharing Reciprocal Reward Multi-Agent Actor-Critic (MAAC-R) algorithm is proposed to learn the cooperative sharing policy for all homogeneous UAVs. Experiments demonstrate that the proposed algorithm can improve the UAVs’ cooperation more effectively than the baseline algorithms, and can stimulate a rich form of cooperative tracking behaviors of UAV swarms. Besides, the learned policy can better scale to other scenarios with more UAVs and targets.}
}
% TODO
@inproceedings{abdiPaletteLoadBalancing2023,
	title        = {{Palette Load Balancing: Locality Hints for Serverless Functions}},
	shorttitle   = {Palette {{Load Balancing}}},
	author       = {Abdi, Mania and Ginzburg, Samuel and Lin, Xiayue Charles and Faleiro, Jose and Chaudhry, Gohar Irfan and Goiri, Inigo and Bianchini, Ricardo and Berger, Daniel S and Fonseca, Rodrigo},
	year         = 2023,
	month        = may,
	booktitle    = {Proceedings of the Eighteenth European Conference on Computer Systems},
    pages        = {365–380},
    numpages     = {16},
    keywords     = {data-parallel processing, caching, serverless computing, cloud computing},
    location     = {Rome, Italy},
    series       = {EuroSys '23},
	publisher    = {Association for Computing Machinery},
    address      = {New York, NY, USA},
	doi          = {10.1145/3552326.3567496},
	isbn         = {978-1-4503-9487-1},
	abstract     = {Function-as-a-Service (FaaS) serverless computing enables a simple programming model with almost unbounded elasticity. Unfortunately, current FaaS platforms achieve this flexibility at the cost of lower performance for data-intensive applications compared to a serverful deployment. The ability to have computation close to data is a key missing feature. We introduce Palette load balancing, which offers FaaS applications a simple mechanism to express locality to the platform, through hints we term ``colors''. Palette maintains the serverless nature of the service \textendash{} users are still not allocating resources \textendash{} while allowing the platform to place successive invocations related to each other on the same executing node. We compare a prototype of the Palette load balancer to a state-of-the-art locality-oblivious load balancer on representative examples of three applications. For a serverless web application with a local cache, Palette improves the hit ratio by 6x. For a serverless version of Dask, Palette improves run times by 46\% and 40\% on Task Bench and TPC-H, respectively. On a serverless version of NumS, Palette improves run times by 37\%. These improvements largely bridge the gap to serverful implementation of the same systems.},
	langid       = {english}
}
@inproceedings{zijunFassflowEfficient2022,
	title        = {{FaaSFlow: Enable Efficient Workflow Execution for Function-as-a-Service}},
	author       = {Li, Zijun and Liu, Yushi and Guo, Linsong and Chen, Quan and Cheng, Jiagan and Zheng, Wenli and Guo, Minyi},
	year         = 2022,
	booktitle    = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
    pages        = {782–796},
    numpages     = {15},
    keywords     = {FaaS, graph partition, master-worker, serverless workflows},
    location     = {Lausanne, Switzerland},
    series       = {ASPLOS '22},
    publisher    = {Association for Computing Machinery},
    address      = {New York, NY, USA},
	doi          = {10.1145/3503222.3507717},
    isbn         = {9781450392051},
	abstract     = {Serverless computing (Function-as-a-Service) provides fine-grain resource sharing by running functions (or Lambdas) in containers. Data-dependent functions are required to be invoked following a pre-defined logic, which is known as serverless workflows. However, our investigation shows that the traditional master-worker based workflow execution architecture performs poorly in serverless context. One significant overhead results from the master-side workflow schedule pattern, with which the functions are triggered in the master node and assigned to worker nodes for execution. Besides, the data movement between workers also reduces the throughput. To this end, we present a worker-side workflow schedule pattern for serverless workflow execution. Following the design, we implement FaaSFlow to enable efficient workflow execution in the serverless context. Besides, we propose an adaptive storage library FaaStore that enables fast data transfer between functions on the same node without through the database. Experiment results show that FaaSFlow effectively mitigates the workflow scheduling overhead by 74.6\% on average and data transmission overhead by 95\% at most. When the network bandwidth fluctuates, FaaSFlow-FaaStore reduces the throughput degradation by 23.0\%, and is able to multiply the utilization of network bandwidth by 1.5X-4X.},
	numpages     = 15,
	keywords     = {graph partition, master-worker, FaaS, serverless workflows}
}
@inproceedings{bhasiCypressInputSizesensitive2022,
	title        = {{Cypress: Input Size-Sensitive Container Provisioning and Request Scheduling for Serverless Platforms}},
	shorttitle   = {Cypress},
	author       = {Bhasi, Vivek M. and Gunasekaran, Jashwant Raj and Sharma, Aakash and Kandemir, Mahmut Taylan and Das, Chita},
	year         = 2022,
	month        = nov,
	booktitle    = {Proceedings of the 13th Symposium on Cloud Computing},
    pages        = {257–272},
    numpages     = {16},
    keywords     = {input size, resource-management, scheduling, serverless},
    location     = {San Francisco, California},
    series       = {SoCC '22},
	publisher    = {Association for Computing Machinery},
    address      = {New York, NY, USA},
	doi          = {10.1145/3542929.3563464},
	isbn         = {978-1-4503-9414-7},
	abstract     = {The growing popularity of the serverless platform has seen an increase in the number and variety of applications (apps) being deployed on it. The majority of these apps process user-provided input to produce the desired results. Existing work in the area of input-sensitive profiling has empirically shown that many such apps have input size\textendash dependent execution times which can be determined through modelling techniques. Nevertheless, existing serverless resource management frameworks are agnostic to the input size\textendash sensitive nature of these apps. We demonstrate in this paper that this can potentially lead to container over-provisioning and/or end-to-end Service Level Objective (SLO) violations. To address this, we propose Cypress, an input size\textendash sensitive resource management framework, that minimizes the containers provisioned for apps, while ensuring a high degree of SLO compliance. We perform an extensive evaluation of Cypress on top of a Kubernetes-managed cluster using 5 apps from the AWS Serverless Application Repository and/or OpenFaaS Function Store with real-world traces and varied input size distributions. Our experimental results show that Cypress spawns up to 66\% fewer containers, thereby, improving container utilization and saving cluster-wide energy by up to 2.95\texttimes{} and 23\%, respectively, versus state-of-the-art frameworks, while remaining highly SLO-compliant (up to 99.99\%). CCS Concepts \textbullet{} Computer systems organization \textrightarrow{} Cloud Computing; Resource-Management; Scheduling.},
	langid       = {english}
}
@inproceedings{zhangFIRSTExploitingMultiDimensional2023,
	title        = {{FIRST: Exploiting the Multi-Dimensional Attributes of Functions for Power-Aware Serverless Computing}},
	shorttitle   = {{FIRST}},
	author       = {Zhang, Lu and Li, Chao and Wang, Xinkai and Feng, Weiqi and Yu, Zheng and Chen, Quan and Leng, Jingwen and Guo, Minyi and Yang, Pu and Yue, Shang},
	year         = 2023,
	month        = may,
	booktitle    = {2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
	publisher    = {{IEEE}},
	address      = {{St. Petersburg, FL, USA}},
	pages        = {864--874},
	doi          = {10.1109/IPDPS54959.2023.00091},
	isbn         = {9798350337662},
	abstract     = {Emerging cloud-native development models raise new challenges for managing server performance and power at microsecond scale. Compared with traditional cloud workloads, serverless functions exhibit unprecedented heterogeneity, variability, and dynamicity. Designing cloud-native power management schemes for serverless functions requires significant engineering effort. Current solutions remain sub-optimal since their orchestration process is often one-sided, lacking a systematic view. A key obstacle to truly efficient function deployment is the fundamental wide abstraction gap between the upper-layer request scheduling and the low-level hardware execution.},
	langid       = {english}
}
@inproceedings{smithFaDOFaaSFunctions2022,
	title        = {{FaDO: FaaS Functions and Data Orchestrator for Multiple Serverless Edge-Cloud Clusters}},
	shorttitle   = {{FaDO}},
	author       = {Smith, Christopher Peter and Jindal, Anshul and Chadha, Mohak and Gerndt, Michael and Benedict, Shajulin},
	year         = 2022,
	month        = may,
	booktitle    = {2022 IEEE 6th International Conference on Fog and Edge Computing (ICFEC)},
	publisher    = {{IEEE}},
	address      = {{Messina, Italy}},
	pages        = {17--25},
	doi          = {10.1109/ICFEC54809.2022.00010},
	isbn         = {978-1-66549-524-0},
	abstract     = {Function-as-a-Service (FaaS) is an attractive cloud computing model that simplifies application development and deployment. However, current serverless compute platforms do not consider data placement when scheduling functions. With the growing demand for edge-cloud continuum, multi-cloud, and multi-serverless applications, this flaw means serverless technologies are still ill-suited to latency-sensitive operations like media streaming. This work proposes a solution by presenting a tool called FaDO: FaaS Functions and Data Orchestrator, designed to allow data-aware functions scheduling across multiserverless compute clusters present at different locations, such as at the edge and in the cloud. FaDO works through headerbased HTTP reverse proxying and uses three load-balancing algorithms: 1) The Least Connections, 2) Round Robin, and 3) Random for load balancing the invocations of the function across the suitable serverless compute clusters based on the set storage policies. FaDO further provides users with an abstraction of the serverless compute cluster's storage, allowing users to interact with data across different storage services through a unified interface. In addition, users can configure automatic and policy-aware granular data replications, causing FaDO to spread data across the clusters while respecting location constraints. Load testing results show that it is capable of load balancing high-throughput workloads, placing functions near their data without contributing any significant performance overhead.},
	langid       = {english}
}
@inproceedings{fuerstIluvatarFastControl2023,
	title        = {{Il\'{u}vatar: A Fast Control Plane for Serverless Computing}},
	author       = {Fuerst, Alexander and Rehman, Abdul and Sharma, Prateek},
	year         = 2023,
	booktitle    = {Proceedings of the 32nd International Symposium on High-Performance Parallel and Distributed Computing},
    pages        = {267–280},
    numpages     = {14},
    keywords     = {cloud computing, functions as a service, open source, serverless computing},
    location     = {Orlando, FL, USA},
    series       = {HPDC '23},
	publisher    = {Association for Computing Machinery},
    address      = {New York, NY, USA},
	doi          = {10.1145/3588195.3592995},
	isbn         = {9798400701559},
	abstract     = {Providing efficient Functions as a Service (FaaS) is challenging due to the serverless programming model and highly heterogeneous and dynamic workloads. Great strides have been made in optimizing FaaS performance through scheduling, caching, virtualization, and other resource management techniques. The combination of these advances and growing FaaS workloads have pushed the performance bottleneck into the control plane itself. Current FaaS control planes like OpenWhisk introduce 100s of milliseconds of latency overhead, and are becoming unsuitable for high performance FaaS research and deployments.We present the design and implementation of Il\'{u}vatar, a fast, modular, extensible FaaS control plane which reduces the latency overhead by more than two orders of magnitude. Il\'{u}vatar has a worker-centric architecture and introduces a new function queue technique for managing function scheduling and overcommitment. Il\'{u}vatar is implemented in Rust in about 13,000 lines of code, and introduces only 3ms of latency overhead under a wide range of loads, which is more than 2 orders of magnitude lower than OpenWhisk.},
	numpages     = 14,
	keywords     = {open source, functions as a service, cloud computing, serverless computing}
}
@article{ei-white-paper,
	title        = {{Edge Intelligence: Paving the Last Mile of Artificial Intelligence With Edge Computing}},
	author       = {Zhou, Zhi and Chen, Xu and Li, En and Zeng, Liekang and Luo, Ke and Zhang, Junshan},
	year         = 2019,
	journal      = {Proceedings of the IEEE},
	volume       = 107,
	number       = 8,
	pages        = {1738--1762},
	doi          = {10.1109/JPROC.2019.2918951}
}
@inproceedings{baller2021deepedgebench,
	title        = {{DeepEdgeBench: Benchmarking deep neural networks on edge devices}},
	author       = {Baller, Stephan Patrick and Jindal, Anshul and Chadha, Mohak and Gerndt, Michael},
	year         = 2021,
	booktitle    = {{IC2E 2021}},
	pages        = {20--30},
	organization = {IEEE}
}
@inproceedings{kljucaric2020,
	title        = {{Architectural Analysis of Deep Learning on Edge Accelerators}},
	author       = {Kljucaric, Luke and Johnson, Alex and George, Alan D.},
	year         = 2020,
	booktitle    = {{HPEC 2020}},
	volume       = {},
	number       = {},
	pages        = {1--7},
	doi          = {10.1109/HPEC43674.2020.9286209}
}
@article{eskandari2020,
	title        = {{Passban IDS: An Intelligent Anomaly-Based Intrusion Detection System for IoT Edge Devices}},
	author       = {Eskandari, Mojtaba and Janjua, Zaffar Haider and Vecchio, Massimo and Antonelli, Fabio},
	year         = 2020,
	journal      = {IEEE Internet of Things Journal},
	volume       = 7,
	number       = 8,
	pages        = {6882--6897},
	doi          = {10.1109/JIOT.2020.2970501}
}
@inproceedings{9928755,
	title        = {{On the Modelling and Analysis of Edge-Serverless Computing}},
	author       = {Li, Shuo and Baştuğ, Ejder and Di Renzo, Marco},
	year         = 2022,
	booktitle    = {{MeditCom 2022}},
	volume       = {},
	number       = {},
	pages        = {250--254},
	doi          = {10.1109/MeditCom55741.2022.9928755}
}
