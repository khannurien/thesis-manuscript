\chapter{Introduction}
\label{chapter:introduction}
%\addcontentsline{toc}{chapter}{Introduction}
%\chaptermark{Introduction}

\section{Contexte scientifique}
%\addcontentsline{toc}{section}{Motivation}

En 1961, John McCarthy donne un discours pour célébrer les cent ans du MIT~\cite{greenberger1962management}. Il imagine alors que le partage du temps de calcul des ordinateurs pourrait permettre de vendre leur puissance d'exécution comme un service, à l'image de l'eau ou de l'électricité. Le matériel serait organisé de manière à rendre possible sa location à des clients qui paieraient ce service en fonction du volume, ou du temps d'utilisation.

Grâce à la démocratisation de l'accès à Internet à haut débit, au milieu des années 2000, l'idée de McCarthy se trouve implémentée dans ce que l'on appelle \textit{cloud computing}~\cite{hayesCloudComputing2008} (ou \textit{cloud}) : entreprises et particuliers peuvent dorénavant réduire drastiquement les coûts associés à l'achat et à l'entretien du matériel nécessaire au fonctionnement de leurs applications, en déléguant la responsabilité de l'infrastructure à des fournisseurs de services, qui bénéficient d'effets d'économie d'échelle en concentrant ces ressources dans d'immenses centres de données.

Ce modèle est appelé \textit{Infrastructure as a Service} (IaaS) : les clients louent et réservent une sous-partie des ressources du fournisseur (calcul, stockage, réseau) dont ils deviennent responsables du fonctionnement~\cite{mellNISTDefinitionCloud}. Dans ce modèle, le client est intégralement en charge du dimensionnement des ressources dédiées à son application ; il pourra avoir tendance à surestimer ses besoins en ressources, de manière à s'assurer que l'application déployée soit capable d'absorber la montée en charge lors de pics d'activité~\cite{takMoveNotMove}. De nouvelles évolutions apparaissent au fil des années avec pour objectif de diminuer la surface des responsabilités du client. Par exemple, dans le modèle \textit{Platform as a Service} (PaaS), les clients n'ont pas directement accès aux machines qui supportent leurs applications, et effectuent la plupart des tâches de gestion via des interfaces spécialisées. Ici, lorsque l'application n'est pas utilisée, elle reste toutefois déployée et occupe donc des ressources matérielles. Enfin, dans une offre \textit{Software as a Service} (SaaS), le fournisseur de services héberge et administre des applications, pour lesquelles il facture au client le droit d'en être utilisateur. Cette facturation est souvent forfaitaire, quelle que soit l'utilisation faite de l'application par le client.

Dans ces trois modèles traditionnels, le client paie pour des ressources qui sont parfois dormantes. En effet, la mesure de l'usage du service s'effectue dans chacun des cas sur la quantité de ressources engagées pour sa mise en œuvre par le fournisseur. C'est un problème largement documenté dans la littérature, qui considère que le taux d'usage des ressources dans un centre de données cloud peut être inférieur à 15\% en moyenne~\cite{vasanWorthTheirWatts2010, vermaLargescaleClusterManagement2015a}.

Cette valeur faible est expliquée par un ensemble de facteurs. D'abord, une part du matériel disponible dans un centre de données est déployée pour palier les éventuelles pannes -- celles-ci peuvent affecter 3\% des machines chaque semaine~\cite{BareMetal70B}. Ensuite, un centre de données est toujours dimensionné par rapport aux pics d'utilisation, qui peuvent être liés à des événements récurrents comme le Super Bowl aux États-Unis~\cite{wangTouchdownCloudImpact2019}, ou imprévisibles comme la crise sanitaire de 2020~\cite{alashhabImpactCoronavirusPandemic2021} : c'est une marge de sécurité, afin d'assurer la continuité du service. Enfin, le cloud est un secteur économique qui connaît une croissance soutenue, autour de 17\% par an~[TODO], ce qui explique des investissements réguliers dans du nouveau matériel pour tenir compte de la demande future.

Malgré tout, lorsque l'on met ce faible usage des ressources en regard de la consommation d'énergie estimée pour ces même centres de données, soit environ 1,5\% de la consommation d'électricité mondiale en 2010~\cite{masanetRecalibratingGlobalData2020} et peut-être jusqu'à 3\% en 2029~[TODO], il semble urgent de s'intéresser à l'optimisation des opérations cloud.

En effet, l'accroissement de l'efficacité des architectures matérielles et logicielles n'a pas permis de contrer un certain effet rebond : si le rapport, pour un serveur typique du cloud, entre énergie consommée et calculs effectués a été divisé par quatre entre 2010 et 2020~\cite{masanetRecalibratingGlobalData2020}, la demande en matériel ne cesse de croître, poussée notamment par les immenses besoins en apprentissage machine~\cite{commentMetaOperate6002024}. Les investissements cumulés des leaders du secteur en serveurs dédiés à l'intelligence artificielle (IA) pourraient quintupler entre 2022 et 2025~\cite{DerriereIADeferlante2024, elderNextWaveAI2024}. L'Agence Internationale de l'Énergie (AIE) estime que la consommation des centres de données pourrait doubler d'ici à 2026~\cite{Electricity2024Analysis2024}.

Une métrique couramment utilisée pour mesurer l'efficacité énergétique d'un centre de données est le PUE (\textit{Power Usage Effectiveness}). Cet indicateur mesure le rapport entre l'énergie totale engagée par le centre de données sur l'énergie engagée pour les opérations (calcul, stockage, etc.), et devrait donc tendre vers $1$ dans un monde idéal. En 2022, le PUE moyen se situe autour de 1,55~\cite{davisUptimeInstituteGlobal2022}, principalement gonflé par les besoins en refroidissement des machines.

Pourtant, un serveur éteint n'a pas besoin d'être refroidi. Comment expliquer alors que les ressources matérielles restent dans un état de stase, augmentant la consommation des centres de données sans générer de revenus ? Ne serait-il pas possible d'envisager un modèle dans lequel les ressources sont allouées au plus proche des besoins, laissant la possibilité aux fournisseurs de services de mettre en œuvre des politiques d'extinction pour les serveurs inutilisés ?

Au cours des années 2010, des acteurs du cloud public proposent de nouvelles déclinaisons de leurs offres commerciales, le modèle \textit{Function as a Service} (FaaS). En particulier, Amazon propose Lambda~\footnote{\href{https://aws.amazon.com/fr/lambda/}{https://aws.amazon.com/fr/lambda/}} en 2014. Avec Lambda, la granularité de réservation des ressources se déplace de la \textit{quantité} vers la \textit{durée} : les clients fournissent à Lambda le code qu'ils souhaitent exécuter sur la plateforme cloud, et les ressources nécessaires à cette exécution sont allouées \textit{lorsque} nécessaire, et \textit{autant} que nécessaire, c'est-à-dire à la hauteur des requêtes utilisateur reçues par l'application. Tant que l'application est inutilisée, les ressources sont libérées. Dès que la charge augmente, les ressources allouées augmentent pour absorber le trafic.

Le déplacement de la responsabilité du dimensionnement des ressources allouées aux applications du client vers le fournisseur de services ouvre de nouvelles possibilités d'optimisation pour ce dernier, en appliquant des stratégies intelligentes de gestion des ressources.

Le fournisseur de services a tout intérêt à optimiser l'orchestration des charges de travail dans ses centres de données. D'une part, cela permet de garantir un niveau satisfaisant de qualité de service aux clients, et donc d'éviter le paiement de pénalités lorsque les accords de niveau de service sont violés. D'autre part, sur les aspects réglementaires, de nouvelles exigences émergent en matière de mesure de la consommation d'énergie et de l'empreinte carbone pour les opérateurs de centres de données~\cite{davisUptimeInstituteGlobal2022}.

Si l'orchestration dynamique apparaît comme étant une solution toute trouvée aux problèmes d'usage des ressources dans le cloud, ce mécanisme amène avec lui un certain nombres de défis à relever~\cite{Lannurien2023}. Pour optimiser, il faut prendre en compte la diversité des ressources à disposition : le cloud est un environnement hautement hétérogène, dans lequel le matériel présente des niveaux de performances et de coût très divers~\cite{reissHeterogeneityDynamicityClouds}. Par ailleurs, il ne s'agit pas uniquement de proposer les performances les plus hautes de manière indiscriminée. Par exemple, de nombreux utilisateurs de services cloud n'ont pas besoin de garanties fortes en matière de latence~\cite{tirmaziBorgNextGeneration2020}. Ainsi, une connaissance fine des utilisateurs et des charges de travail est requise pour mettre en œuvre des politiques pertinentes de répartition de charge ou de consolidation des tâches.

\section{Problématiques de recherche}
%\addcontentsline{toc}{section}{Problématiques de recherche}

Les travaux menés au cours de cette thèse s'inscrivent dans le cadre du modèle de service serverless pour le cloud privé. Nous avons considéré un environnement dans lequel un fournisseur de services dispose de ressources matérielles que des développeurs peuvent exploiter pour déployer leurs applications à destination d'une variété d'utilisateurs. Nous avons cherché à concevoir une plateforme d'orchestration qui soit en mesure de garantir la qualité de service pour les utilisateurs tout en minimisant les coûts pour le fournisseur de services. Une progression s'est opérée, en partant de l'ordonnancement de fonctions simples (sans dépendances et sans état), pour arriver au déploiement d'applications complexes (dépendances temporelles et de données entre les fonctions des applications). Il nous a également fallu développer un environnement d'évaluation des différentes politiques que nous avons proposées.

Cette section résume les problèmes que nous avons identifiés et les questions de recherche que nous avons posées dans le but de lever ces verrous.

\begin{center}
    \rule{4cm}{0.4pt}
\end{center}

\textbf{Problème 1} -- L'allocation de ressources au sein d'une plateforme serverless se fait sur un mode réactif, c'est-à-dire en réponse à une variation du trafic. Lorsque de nouvelles instances des fonctions doivent être déployées, cela provoque une latence additionnelle qui peut dégrader la qualité de service pour les utilisateurs. Le problème est particulièrement saillant dans l'environnement hautement hétérogène qu'est celui du cloud : les ressources matérielles ont différents niveaux de performances et de coût ; les applications interactives présentent des motifs d'usage difficilement prédictibles ; et les utilisateurs ont des besoins irréguliers et variés en qualité de service.

\boitemagique{Question 1 (\textbf{QR1})}{
    Comment dimensionner les allocations dynamiques de ressources hétérogènes pour une application interactive, et comment ordonnancer efficacement les requêtes utilisateur, lorsque ces derniers ont des besoins variés en matière de qualité de service ?
}

\begin{center}
    \rule{4cm}{0.4pt}
\end{center}

\textbf{Problème 2} -- Ordonnancer des applications complexes, constituées d'un ensemble de fonctions qui présentent des relations de dépendances et communiquent entre elles des résultats intermédiaires, soulève le problème du stockage dans le modèle serverless. Les solutions commerciales s'appuient sur un stockage lent, accédé par le réseau, qui peut provoquer des délais à chaque étage de l'application. Ces délais s'accumulent alors et provoquent un effet boule de neige, dégradant drastiquement la qualité de service. Une possibilité existe d'exploiter le stockage local aux nœuds de calcul, mais induit un risque de contention sur les ressources, qui peuvent être limitées en capacité et en performances dans le cadre de l'informatique en périphérie (ou \textit{edge}).

\boitemagique{Question 2 (\textbf{QR2})}{
    Comment prendre en compte les délais d'initialisation et de communications lorsque l'on déploie des chaînes de fonctions de courte durée, et comment tirer parti de l'hétérogénéité des nœuds à disposition, pour respecter la qualité de service requise par les utilisateurs et contenir la consommation d'énergie de l'infrastructure ?
}

\begin{center}
    \rule{4cm}{0.4pt}
\end{center}

\textbf{Problème 3} -- Évaluer des politiques d'orchestration pour un environnement cloud privé peut se faire de deux manières différentes : par expérimentation, c'est-à-dire avec du matériel et des utilisateurs réels, ou en simulation. La méthode expérimentale se heurte à des contraintes de coût d'une part, car il faut réserver une partie de l'infrastructure pour mener les campagnes de mesure. D'autre part, il est complexe de mener une analyse \textit{what-if} en production, c'est-à-dire de se laisser la liberté de changer un certain nombre de paramètres en entrée pour évaluer leur impact (fréquence des requêtes, caractéristiques du matériel, etc.). Les simulateurs à l'état de l'art pour le cloud, quant à eux, présentent des limites qui rendent délicate la modélisation d'un environnement serverless permettant de comparer les performances de différentes stratégies de gestion des ressources.

\boitemagique{Question 3 (\textbf{QR3})}{
    Du point de vue d'un fournisseur de services pour le cloud privé, comment évaluer l'impact sur la qualité de service de différentes politiques d'allocation de ressources et d'ordonnancement de tâches dans le modèle serverless ?
}

\section{Contributions}
%\addcontentsline{toc}{section}{Contributions}

\subsection{Caractérisation et orchestration de tâches simples pour la détection de deepfake}

La \textbf{QR1} (\textit{dimensionnement et ordonnancement sur ressources hétérogènes}) a émergé d'une réflexion autour du déploiement d'une application interactive sur des ressources non-réservées au sein de l'IRT b{\textless\textgreater}com~\footnote{\href{https://b-com.com/}{https://b-com.com/}}. Notre cas d'étude concerne une application de détection de deepfake -- des images générées dans un but malicieux par une machine, avec l'objectif de tromper leur destinataire. Cette application est réveillée par l'envoi, par un utilisateur, d'une image à classifier. Elle s'appuie sur des algorithmes d'apprentissage machine pour déterminer si l'image est en effet synthétique ou bien légitime.

Pour répondre à la \textbf{QR1}, nous avons proposé HeROfake (chapitre~\ref{chapter:herofake}), une plateforme d'orchestration serverless qui vise à réaliser de manière intelligente l'allocation des ressources et l'ordonnancement des requêtes, dans le but d'optimiser l'orchestration pour la qualité de service tout en minimisant la consommation d'énergie du système. Nous avons mené une campagne de mesures afin de caractériser, d'une part, un ensemble de ressources matérielles hétérogènes (CPU, GPU, FPGA), et d'autre part, un ensemble de fonctions d'inférence utilisées par l'application et basées sur des réseaux de neurones. Cette phase de caractérisation permet d'obtenir des métadonnées sur le niveau de performances et de consommation de ressources pour l'application.

Ces métadonnées sont ensuite utilisées par l'orchestrateur lors d'une phase en ligne, afin d'ajuster ses décisions d'allocation et d'ordonnancement tout au long d'un scénario, rejoué en simulation dans un environnement ad-hoc. Nous avons établi un modèle de coût pour cet orchestrateur, puis conçu une politique d'optimisation multi-objectifs visant à minimiser ces fonctions de coût, nourries par les métadonnées issues de la phase de mesures.

Cette contribution a fait l'objet d'une publication dans la conférence CCGrid'23~\cite{herofake}.

\subsection{Modèle de coût pour l'orchestration serverless d'applications complexes à l'\textit{edge}}

La \textbf{QR2} (\textit{prise en compte des coûts associés au stockage dans le serverless}) a émergé pour deux raisons principales. D'une part, comme la suite logique des travaux menés sur la première contribution : si notre précédente contribution permet d'orchestrer des fonctions sans état, nous savons que la plupart des applications ont besoin de stocker des données intermédiaires pendant leur exécution. D'autre part, une étude de la littérature en matière d'orchestration serverless nous a permis de constater que de nombreuses contributions prenaient souvent comme point de départ une situation très favorable, considérant que les images servant à déployer les fonctions sont toujours disponibles sur les nœuds de calcul, et que les communications entre les fonctions sont transparentes du point de vue des performances.

Pour répondre à la \textbf{QR2}, nous avons cherché à établir un modèle de coût pour l'allocation de ressources et l'ordonnancement des tâches qui prenne en compte ces deux mécanismes liés au stockage. Notre cas d'étude pour cette contribution est une application de détection d'instrusions, déployée à l'edge, sur des nœuds hétérogènes aux ressources contraintes et limités en nombre par l'énergie disponible. Nous avons proposé HeROcache (chapitre~\ref{chapter:herocache}), une stratégie d'orchestration qui cherche à maximiser la consolidation des fonctions d'une même application sur les mêmes nœuds, afin de minimiser la consommation d'énergie de l'infrastructure. Grâce à un mécanisme de mise en cache des données sur le stockage local, cette stratégie permet de réduire les délais de communication des applications et limite les violations de qualité de service.

Cette contribution a fait l'objet d'une publication dans la conférence CCGrid'24~\cite{herocache}.

\subsection{Simulation à événements discrets pour l'évaluation de politiques d'orchestration}

% données de production (traces d'exécution), analyse (graphe des appels, données intermédiaires), caractérisation (métriques de performances)

La \textbf{QR3} (\textit{évaluer et comparer différentes politiques d'orchestration serverless}) a été transversale tout au long des travaux de thèse. Concevoir des stratégies d'allocation et d'ordonnancement pour le cloud demande d'avoir à notre disposition un environnement dans lequel les évaluer.

Pour répondre à la \textbf{QR3}, nous avons développé et proposé HeROsim (chapitre~\ref{chapter:herosim}), un simulateur \textit{open source} à événements discrets pour le cloud privé serverless, qui présente les caractéristiques nécessaires à la modélisation fine d'un environnement serverless et l'évaluation de politiques d'orchestration. La simulation progresse à une granularité au niveau d'une requête utilisateur, et permet de modéliser des applications complexes (dépendances entre les fonctions) et des environnements hétérogènes (nœuds à différents niveaux de performances, requêtes à différents niveaux de qualité de service). HeROsim permet de rejouer des scénarios (sur la base de traces d'exécution) et de comparer les résultats de différentes politiques au regard de métriques de qualité de service et de consommation d'énergie.

Cette contribution a fait l'objet d'une soumission au journal IEEE Internet Computing~\cite{herosim} en mai 2024.

\clearpage

\section{Organisation de la thèse}
%\addcontentsline{toc}{section}{Organisation du mémoire}

Ce manuscrit est composé de sept chapitres (dont cette introduction), organisés comme suit :

\begin{center}
    \rule{4cm}{0.4pt}
\end{center}

\textbf{Partie~\ref{part:one} : Contexte et état de l'art}

Le chapitre~\ref{chapter:context} décrit l'environnement général des travaux de la thèse, et donne en particulier les caractéristiques du cloud et du modèle serverless. Le chapitre~\ref{chapter:sota} présente des contributions de l'état de l'art dans le domaine de l'orchestration dynamique pour le cloud.

\begin{center}
    \rule{4cm}{0.4pt}
\end{center}

\textbf{Partie~\ref{part:two} : Contributions}

Cette partie présente les trois contributions de la thèse. Le chapitre~\ref{chapter:herofake} décrit notre solution d'allocation et d'ordonnancement dynamiques sous contrainte de qualité de service pour des fonctions simples, ainsi que notre méthodologie de caractérisation des plateformes et des tâches. Le chapitre~\ref{chapter:herocache} présente notre orchestrateur s'appuyant sur un modèle de coût intégrant les problématiques de stockage pour le serverless. Le chapitre~\ref{chapter:herocache} détaille l'environnement de simulation développé dans le cadre de la thèse pour évaluer des politiques d'orchestration serverless pour le cloud privé.

\begin{center}
    \rule{4cm}{0.4pt}
\end{center}

\textbf{Partie~\ref{part:three} : Conclusion et perspectives}

Enfin, le chapitre~\ref{chapter:conclusion} résume les contributions de la thèse et présente les limites identifiées dans les solutions proposées, pour ouvrir la discussion sur des perspectives de futurs travaux.
